{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec1de3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/Public/kheed/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-07 10:56:40,780 - INFO - Launching Gradio interface\n",
      "2025-08-07 10:56:40,830 - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 10:56:40,839 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 10:56:41,662 - INFO - HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 10:56:42,153 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://d0dcf751121b884bdb.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 10:56:44,089 - INFO - HTTP Request: HEAD https://d0dcf751121b884bdb.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d0dcf751121b884bdb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import XLMRobertaForTokenClassification, XLMRobertaTokenizerFast, pipeline\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('ner_interface.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define paths\n",
    "MODEL_PATH = \"/home/guest/Public/KHEED/KHEED_Data_Collection/xlmr_ner_model/final_model\"\n",
    "LABEL2ID_PATH = \"/home/guest/Public/KHEED/KHEED_Data_Collection/xlmr_ner_model/label2id.json\"\n",
    "\n",
    "# Define colors for entity types\n",
    "ENTITY_COLORS = {\n",
    "    \"Disease\": \"red\",\n",
    "    \"Organization\": \"blue\",\n",
    "    \"Location\": \"green\",\n",
    "    \"Date\": \"yellow\",\n",
    "    \"AffectedCount\": \"purple\",\n",
    "    \"Medication\": \"orange\",\n",
    "    \"Symptom\": \"pink\",\n",
    "    \"Pathogen\": \"cyan\"\n",
    "}\n",
    "\n",
    "def load_model_and_tokenizer() -> Tuple[XLMRobertaForTokenClassification, XLMRobertaTokenizerFast, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Load the fine-tuned model, tokenizer, and label mappings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Loading tokenizer from %s\", MODEL_PATH)\n",
    "        tokenizer = XLMRobertaTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "        logger.info(\"Tokenizer loaded successfully\")\n",
    "\n",
    "        logger.info(\"Loading model from %s\", MODEL_PATH)\n",
    "        model = XLMRobertaForTokenClassification.from_pretrained(MODEL_PATH)\n",
    "        logger.info(\"Model loaded successfully\")\n",
    "\n",
    "        logger.info(\"Loading label mappings from %s\", LABEL2ID_PATH)\n",
    "        with open(LABEL2ID_PATH, 'r', encoding='utf-8') as f:\n",
    "            label2id = json.load(f)\n",
    "        id2label = {int(k): v for k, v in model.config.id2label.items()}\n",
    "        logger.info(\"Label mappings loaded successfully\")\n",
    "\n",
    "        return model, tokenizer, label2id, id2label\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error loading model/tokenizer/label mappings: %s\", str(e))\n",
    "        raise\n",
    "\n",
    "def process_entities(entities: List[Dict], input_text: str) -> Tuple[List[Tuple], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Process raw NER pipeline output to prepare data for Gradio.\n",
    "    \"\"\"\n",
    "    processed_entities = []\n",
    "    highlighted_text = []\n",
    "    last_end = 0\n",
    "\n",
    "    for entity in entities:\n",
    "        entity_type = entity['entity_group']\n",
    "        score = float(entity['score'])\n",
    "        word = entity['word']\n",
    "        start = entity['start']\n",
    "        end = entity['end']\n",
    "\n",
    "        # Add non-entity text before the current entity\n",
    "        if start > last_end:\n",
    "            highlighted_text.append((input_text[last_end:start], None))\n",
    "\n",
    "        # Add the entity\n",
    "        highlighted_text.append((input_text[start:end], entity_type))\n",
    "\n",
    "        processed_entities.append({\n",
    "            \"text\": word,\n",
    "            \"type\": entity_type,\n",
    "            \"score\": score\n",
    "        })\n",
    "\n",
    "        last_end = end\n",
    "\n",
    "    # Add any remaining non-entity text\n",
    "    if last_end < len(input_text):\n",
    "        highlighted_text.append((input_text[last_end:], None))\n",
    "\n",
    "    return highlighted_text, processed_entities\n",
    "\n",
    "def perform_ner(text: str) -> Tuple[gr.HighlightedText, gr.DataFrame]:\n",
    "    \"\"\"\n",
    "    Perform NER on input text and return highlighted text and entity table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not text.strip():\n",
    "            logger.warning(\"Empty input text provided\")\n",
    "            return (\n",
    "                [(\"\", None)],\n",
    "                pd.DataFrame({\"Entity\": [], \"Type\": [], \"Confidence\": []})\n",
    "            )\n",
    "\n",
    "        # Load model, tokenizer, and label mappings\n",
    "        model, tokenizer, label2id, id2label = load_model_and_tokenizer()\n",
    "\n",
    "        # Initialize NER pipeline\n",
    "        logger.info(\"Initializing NER pipeline\")\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        if device == 0:\n",
    "            logger.info(\"Device set to use cuda:0\")\n",
    "        else:\n",
    "            logger.info(\"Device set to use CPU\")\n",
    "        ner_pipeline = pipeline(\n",
    "            \"ner\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            aggregation_strategy=\"simple\",\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Run NER pipeline\n",
    "        logger.info(\"Running NER pipeline on input text\")\n",
    "        entities = ner_pipeline(text)\n",
    "        logger.info(\"Raw pipeline output: %s\", entities)\n",
    "\n",
    "        if not entities:\n",
    "            logger.warning(\"No entities detected in the input text\")\n",
    "            return (\n",
    "                [(text, None)],\n",
    "                pd.DataFrame({\"Entity\": [], \"Type\": [], \"Confidence\": []})\n",
    "            )\n",
    "\n",
    "        # Process entities\n",
    "        highlighted_text, processed_entities = process_entities(entities, text)\n",
    "\n",
    "        # Prepare DataFrame\n",
    "        df_data = [{\n",
    "            \"Entity\": entity['text'],\n",
    "            \"Type\": entity['type'],\n",
    "            \"Confidence\": f\"{entity['score']:.4f}\"\n",
    "        } for entity in processed_entities]\n",
    "\n",
    "        df = pd.DataFrame(df_data)\n",
    "\n",
    "        logger.info(\"Processed %d entities\", len(processed_entities))\n",
    "        return (\n",
    "            highlighted_text,\n",
    "            df\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error during NER processing: %s\", str(e))\n",
    "        return (\n",
    "            [(text, None)],\n",
    "            pd.DataFrame({\"Entity\": [], \"Type\": [], \"Confidence\": []})\n",
    "        )\n",
    "\n",
    "# Define default example text\n",
    "DEFAULT_TEXT = \"\"\"\n",
    "នៅខេត្តសៀមរាប៖​ ជំងឺរលាកសួតបានរកឃើញនៅថ្ងៃទី៥ ខែមករា ឆ្នាំ២០២៥ ដោយមានអ្នកជំងឺ៥៦នាក់ ប្រើថ្នាំអាស្ពីរីន និងមានរោគសញ្ញាក្អក និងគ្រុនក្តៅ បណ្តាលមកពីមេរោគអេសអិន១១។\n",
    "\"\"\"\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"Khmer NER Interface\") as demo:\n",
    "    gr.Markdown(\"# Khmer Named Entity Recognition (NER)\")\n",
    "    gr.Markdown(\"Enter Khmer text to identify entities such as Disease, Location, Date, etc.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        input_text = gr.Textbox(\n",
    "            label=\"Input Text\",\n",
    "            value=DEFAULT_TEXT,\n",
    "            lines=5,\n",
    "            placeholder=\"Enter Khmer text here...\"\n",
    "        )\n",
    "    \n",
    "    submit_button = gr.Button(\"Perform NER\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        highlighted_output = gr.HighlightedText(\n",
    "            label=\"Highlighted Entities\",\n",
    "            show_legend=True,\n",
    "            color_map=ENTITY_COLORS\n",
    "        )\n",
    "        entity_table = gr.DataFrame(\n",
    "            label=\"Detected Entities\",\n",
    "            headers=[\"Entity\", \"Type\", \"Confidence\"]\n",
    "        )\n",
    "    \n",
    "    submit_button.click(\n",
    "        fn=perform_ner,\n",
    "        inputs=input_text,\n",
    "        outputs=[highlighted_output, entity_table]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(\"Launching Gradio interface\")\n",
    "        demo.launch(share=True)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to launch Gradio interface: %s\", str(e))\n",
    "        print(f\"Error launching interface: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c491e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 13:14:20,843 - INFO - Launching Gradio interface\n",
      "2025-08-07 13:14:20,884 - INFO - HTTP Request: GET http://127.0.0.1:7875/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 13:14:20,887 - INFO - HTTP Request: HEAD http://127.0.0.1:7875/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 13:14:21,894 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 13:14:21,907 - INFO - HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://c0eb0ee942141099a9.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 13:14:25,869 - INFO - HTTP Request: HEAD https://c0eb0ee942141099a9.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c0eb0ee942141099a9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 13:14:42,521 - INFO - Using PrahokBART model\n",
      "Some weights of MBartModel were not initialized from the model checkpoint at nict-astrec-att/prahokbart_base and are newly initialized: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight', 'shared.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-07 13:14:44,581 - INFO - PrahokBART model loaded successfully\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "2025-08-07 13:14:44,607 - WARNING - Invalid I- tag found at position 10: I-Date. Converting to B-Date.\n",
      "2025-08-07 13:14:44,608 - WARNING - Invalid I- tag found at position 21: I-HumanCount. Converting to B-HumanCount.\n",
      "2025-08-07 13:14:44,608 - WARNING - Invalid I- tag found at position 39: I-Pathogen. Converting to B-Pathogen.\n",
      "2025-08-07 13:14:44,608 - INFO - Processed 10 entities\n",
      "2025-08-07 13:15:12,200 - INFO - Using PrahokBART model\n",
      "Some weights of MBartModel were not initialized from the model checkpoint at nict-astrec-att/prahokbart_base and are newly initialized: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight', 'shared.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-07 13:15:13,750 - INFO - PrahokBART model loaded successfully\n",
      "2025-08-07 13:15:13,776 - WARNING - Invalid I- tag found at position 10: I-Date. Converting to B-Date.\n",
      "2025-08-07 13:15:13,777 - WARNING - Invalid I- tag found at position 21: I-HumanCount. Converting to B-HumanCount.\n",
      "2025-08-07 13:15:13,777 - WARNING - Invalid I- tag found at position 39: I-Pathogen. Converting to B-Pathogen.\n",
      "2025-08-07 13:15:13,777 - INFO - Processed 10 entities\n",
      "2025-08-07 13:15:41,264 - INFO - Using PrahokBART model\n",
      "Some weights of MBartModel were not initialized from the model checkpoint at nict-astrec-att/prahokbart_base and are newly initialized: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight', 'shared.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-07 13:15:42,341 - INFO - PrahokBART model loaded successfully\n",
      "2025-08-07 13:15:42,364 - WARNING - Invalid I- tag found at position 13: I-Date. Converting to B-Date.\n",
      "2025-08-07 13:15:42,365 - WARNING - Invalid I- tag found at position 24: I-HumanCount. Converting to B-HumanCount.\n",
      "2025-08-07 13:15:42,365 - WARNING - Invalid I- tag found at position 49: I-HumanCount. Converting to B-HumanCount.\n",
      "2025-08-07 13:15:42,365 - INFO - Processed 10 entities\n",
      "2025-08-07 13:20:09,672 - INFO - Using PrahokBART model\n",
      "Some weights of MBartModel were not initialized from the model checkpoint at nict-astrec-att/prahokbart_base and are newly initialized: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight', 'shared.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-07 13:20:11,062 - INFO - PrahokBART model loaded successfully\n",
      "2025-08-07 13:20:11,090 - WARNING - Invalid I- tag found at position 12: I-Date. Converting to B-Date.\n",
      "2025-08-07 13:20:11,090 - WARNING - Invalid I- tag found at position 22: I-HumanCount. Converting to B-HumanCount.\n",
      "2025-08-07 13:20:11,091 - WARNING - Invalid I- tag found at position 43: I-Pathogen. Converting to B-Pathogen.\n",
      "2025-08-07 13:20:11,091 - WARNING - Invalid I- tag found at position 44: I-HumanCount. Converting to B-HumanCount.\n",
      "2025-08-07 13:20:11,091 - INFO - Processed 10 entities\n",
      "2025-08-07 13:20:21,876 - INFO - Using PrahokBART model\n",
      "Some weights of MBartModel were not initialized from the model checkpoint at nict-astrec-att/prahokbart_base and are newly initialized: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight', 'shared.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-07 13:20:22,853 - INFO - PrahokBART model loaded successfully\n",
      "2025-08-07 13:20:22,882 - WARNING - Invalid I- tag found at position 12: I-Date. Converting to B-Date.\n",
      "2025-08-07 13:20:22,882 - WARNING - Invalid I- tag found at position 22: I-HumanCount. Converting to B-HumanCount.\n",
      "2025-08-07 13:20:22,883 - INFO - Processed 9 entities\n",
      "2025-08-07 13:20:47,220 - INFO - Using XLM-RoBERTa model\n",
      "2025-08-07 13:20:47,220 - INFO - Loading XLM-RoBERTa tokenizer from /home/guest/Public/KHEED/KHEED_Data_Collection/xlmr_ner_model/final_model\n",
      "2025-08-07 13:20:47,486 - INFO - Loading XLM-RoBERTa model from /home/guest/Public/KHEED/KHEED_Data_Collection/xlmr_ner_model/final_model\n",
      "2025-08-07 13:20:47,510 - INFO - Tokenizer and model loaded successfully\n",
      "2025-08-07 13:20:47,510 - INFO - Loading label mappings from /home/guest/Public/KHEED/KHEED_Data_Collection/xlmr_ner_model/label2id.json\n",
      "2025-08-07 13:20:47,510 - INFO - Label mappings loaded successfully\n",
      "2025-08-07 13:20:47,511 - INFO - Device set to CPU\n",
      "Device set to use cpu\n",
      "2025-08-07 13:20:47,511 - INFO - Running XLM-RoBERTa NER pipeline\n",
      "2025-08-07 13:20:47,539 - INFO - Processed 9 entities\n",
      "2025-08-07 13:21:10,447 - INFO - Using BiLSTM-CRF model\n",
      "2025-08-07 13:21:10,448 - INFO - Loading vocabularies from /home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/bilstm_crf_ner_model/vocabularies.pkl\n",
      "2025-08-07 13:21:10,454 - INFO - Loading BiLSTM-CRF model from /home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/bilstm_crf_ner_model/khmer_ner_best.pt\n",
      "2025-08-07 13:21:10,459 - INFO - BiLSTM-CRF model and vocabularies loaded successfully\n",
      "2025-08-07 13:21:10,462 - WARNING - Invalid I- tag found at position 0: I-Location. Converting to B-Location.\n",
      "2025-08-07 13:21:10,462 - WARNING - Invalid I- tag found at position 9: I-Location. Converting to B-Location.\n",
      "2025-08-07 13:21:10,463 - INFO - Processed 2 entities\n",
      "2025-08-07 13:21:36,433 - INFO - Using BiLSTM-CRF model\n",
      "2025-08-07 13:21:36,434 - INFO - Loading vocabularies from /home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/bilstm_crf_ner_model/vocabularies.pkl\n",
      "2025-08-07 13:21:36,440 - INFO - Loading BiLSTM-CRF model from /home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/bilstm_crf_ner_model/khmer_ner_best.pt\n",
      "2025-08-07 13:21:36,445 - INFO - BiLSTM-CRF model and vocabularies loaded successfully\n",
      "2025-08-07 13:21:36,446 - WARNING - Invalid I- tag found at position 0: I-Location. Converting to B-Location.\n",
      "2025-08-07 13:21:36,446 - WARNING - Invalid I- tag found at position 9: I-Location. Converting to B-Location.\n",
      "2025-08-07 13:21:36,447 - INFO - Processed 2 entities\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import BertForTokenClassification, BertTokenizerFast, XLMRobertaForTokenClassification, XLMRobertaTokenizerFast, pipeline, MBartTokenizerFast, MBartModel\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from khmernltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "try:\n",
    "    from transformers import MBartForTokenClassification\n",
    "except ImportError:\n",
    "    from transformers import MBartModel\n",
    "    import torch.nn as nn\n",
    "\n",
    "    class MBartForTokenClassificationCustom(MBartModel):\n",
    "        def __init__(self, config):\n",
    "            super().__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.classifier = nn.Linear(config.d_model, config.num_labels)\n",
    "        \n",
    "        def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "            outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "            sequence_output = outputs[0]\n",
    "            logits = self.classifier(sequence_output)\n",
    "            \n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "            return {\"logits\": logits, \"loss\": loss} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "# Add the PrahokBARTForNER class from your training code\n",
    "class PrahokBARTForNER(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        self.mbart = MBartModel.from_pretrained(model_name)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.classifier = torch.nn.Linear(self.mbart.config.d_model, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.mbart(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('ner_interface.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define paths\n",
    "XLM_FINAL_MODEL_PATH = \"/home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/xlmr_kh_ner_model/final_model\"\n",
    "XLM_MODEL_PATH = \"/home/guest/Public/KHEED/KHEED_Data_Collection/xlmr_ner_model/final_model\"\n",
    "BERT_MODEL_PATH = \"/home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/Models/bert_khmer_ner_model/final_model\"\n",
    "PRAHOKBART_MODEL_PATH = \"/home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/prahokbart_ner_model\"\n",
    "LABEL2ID_PATH = \"/home/guest/Public/KHEED/KHEED_Data_Collection/xlmr_ner_model/label2id.json\"\n",
    "BILSTM_MODEL_PATH = \"/home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/bilstm_crf_ner_model/khmer_ner_best.pt\"\n",
    "BILSTM_VOCAB_PATH = \"/home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/bilstm_crf_ner_model/vocabularies.pkl\"\n",
    "\n",
    "# Define colors for entity types (unchanged)\n",
    "ENTITY_COLORS = {\n",
    "    \"Disease\": \"red\",\n",
    "    \"Organization\": \"blue\",\n",
    "    \"Location\": \"green\",\n",
    "    \"Date\": \"yellow\",\n",
    "    \"HumanCount\": \"purple\",\n",
    "    \"Medication\": \"orange\",\n",
    "    \"Symptom\": \"pink\",\n",
    "    \"Pathogen\": \"cyan\",\n",
    "}\n",
    "\n",
    "\n",
    "# Map BiLSTM-CRF BIO tags to XLM-RoBERTa entity types\n",
    "BIO_TO_ENTITY = {\n",
    "    \"B-Disease\": \"Disease\",\n",
    "    \"I-Disease\": \"Disease\",\n",
    "    \"B-Organization\": \"Organization\",\n",
    "    \"I-Organization\": \"Organization\",\n",
    "    \"B-Location\": \"Location\",\n",
    "    \"I-Location\": \"Location\",\n",
    "    \"B-Date\": \"Date\",\n",
    "    \"I-Date\": \"Date\",\n",
    "    \"B-HumanCount\": \"HumanCount\",\n",
    "    \"I-HumanCount\": \"HumanCount\",\n",
    "    \"B-Medication\": \"Medication\",\n",
    "    \"I-Medication\": \"Medication\",\n",
    "    \"B-Symptom\": \"Symptom\",\n",
    "    \"I-Symptom\": \"Symptom\",\n",
    "    \"B-Pathogen\": \"Pathogen\",\n",
    "    \"I-Pathogen\": \"Pathogen\",\n",
    "    \"O\": None\n",
    "}\n",
    "\n",
    "# BiLSTM-CRF Model Definition\n",
    "class BiLSTM_CRF(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, dropout=0.5, \n",
    "                 tag_to_idx=None, pretrained_embeddings=None):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight = torch.nn.Parameter(torch.tensor(pretrained_embeddings, dtype=torch.float32))\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
    "        self.transitions = torch.nn.Parameter(torch.randn(tagset_size, tagset_size))\n",
    "        self.tagset_size = tagset_size\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def forward(self, sentences, mask):\n",
    "        emissions = self._get_lstm_features(sentences, mask)\n",
    "        return self._viterbi_decode(emissions, mask)\n",
    "\n",
    "    def _get_lstm_features(self, sentences, mask):\n",
    "        embeds = self.embedding(sentences)\n",
    "        packed_embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, mask.sum(1).cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_lstm_out, _ = self.lstm(packed_embeds)\n",
    "        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_lstm_out, batch_first=True, total_length=embeds.size(1))\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        return emissions\n",
    "\n",
    "    def _viterbi_decode(self, emissions, mask):\n",
    "        tag_seq = emissions.argmax(-1).cpu().numpy()\n",
    "        return None, tag_seq\n",
    "\n",
    "def load_bilstm_model_and_vocab() -> Tuple[BiLSTM_CRF, Dict[str, int], Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"Load the BiLSTM-CRF model and vocabularies.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Loading vocabularies from %s\", BILSTM_VOCAB_PATH)\n",
    "        with open(BILSTM_VOCAB_PATH, 'rb') as f:\n",
    "            vocab_data = pickle.load(f)\n",
    "            word_to_idx = vocab_data['word_to_idx']\n",
    "            tag_to_idx = vocab_data['tag_to_idx']\n",
    "            idx_to_tag = vocab_data['idx_to_tag']\n",
    "        \n",
    "        model = BiLSTM_CRF(\n",
    "            vocab_size=len(word_to_idx),\n",
    "            tagset_size=len(tag_to_idx),\n",
    "            embedding_dim=300,\n",
    "            hidden_dim=256,\n",
    "            dropout=0.5,\n",
    "            tag_to_idx=tag_to_idx,\n",
    "            pretrained_embeddings=None\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Loading BiLSTM-CRF model from %s\", BILSTM_MODEL_PATH)\n",
    "        checkpoint = torch.load(BILSTM_MODEL_PATH, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        logger.info(\"BiLSTM-CRF model and vocabularies loaded successfully\")\n",
    "        return model, word_to_idx, tag_to_idx, idx_to_tag\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error loading BiLSTM-CRF model/vocab: %s\", str(e))\n",
    "        raise\n",
    "\n",
    "def load_prahokbart_model(model_path: str) -> Tuple[PrahokBARTForNER, Dict[str, int], Dict[int, str], Dict[str, int]]:\n",
    "    \"\"\"Load PrahokBART model and vocabularies.\"\"\"\n",
    "    try:\n",
    "        # Load vocabularies\n",
    "        with open(os.path.join(model_path, \"tag2idx.json\"), 'r', encoding='utf-8') as f:\n",
    "            tag2idx = json.load(f)\n",
    "        \n",
    "        with open(os.path.join(model_path, \"word2idx.json\"), 'r', encoding='utf-8') as f:\n",
    "            word2idx = json.load(f)\n",
    "        \n",
    "        idx2tag = {int(idx): tag for tag, idx in tag2idx.items()}\n",
    "        \n",
    "        # Initialize model\n",
    "        model = PrahokBARTForNER(\n",
    "            model_name=\"nict-astrec-att/prahokbart_base\",\n",
    "            num_labels=len(tag2idx)\n",
    "        )\n",
    "        \n",
    "        # Load model weights\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model_weights_path = os.path.join(model_path, \"prahokbart_ner.pt\")\n",
    "        model.load_state_dict(torch.load(model_weights_path, map_location=device))\n",
    "        model.eval()\n",
    "        \n",
    "        logger.info(\"PrahokBART model loaded successfully\")\n",
    "        return model, tag2idx, idx2tag, word2idx\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error loading PrahokBART model: %s\", str(e))\n",
    "        raise\n",
    "\n",
    "def tokenize_khmer_text(text: str) -> List[str]:\n",
    "    \"\"\"Simple whitespace-based tokenizer for Khmer text.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def preprocess_bilstm_input(text: str, word_to_idx: Dict[str, int], max_len: int = 128) -> Tuple[torch.Tensor, torch.Tensor, List[str]]:\n",
    "    \"\"\"Preprocess input text for BiLSTM-CRF model.\"\"\"\n",
    "    tokens = tokenize_khmer_text(text)\n",
    "    word_indices = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n",
    "    \n",
    "    seq_len = min(len(word_indices), max_len)\n",
    "    attention_mask = [True] * seq_len + [False] * (max_len - seq_len)\n",
    "    if len(word_indices) < max_len:\n",
    "        word_indices += [word_to_idx['<PAD>']] * (max_len - len(word_indices))\n",
    "    else:\n",
    "        word_indices = word_indices[:max_len]\n",
    "    \n",
    "    word_tensor = torch.tensor([word_indices], dtype=torch.long)\n",
    "    mask_tensor = torch.tensor([attention_mask], dtype=torch.bool)\n",
    "    \n",
    "    return word_tensor, mask_tensor, tokens[:seq_len]\n",
    "\n",
    "def preprocess_prahokbart_input(text: str, word2idx: Dict[str, int], max_len: int = 128) -> Tuple[torch.Tensor, torch.Tensor, List[str]]:\n",
    "    \"\"\"Preprocess input text for PrahokBART model.\"\"\"\n",
    "    # Tokenize with khmernltk to match training\n",
    "    khmer_tokens = word_tokenize(text)\n",
    "    \n",
    "    # Convert tokens to input IDs using word2idx\n",
    "    input_ids = []\n",
    "    for token in khmer_tokens:\n",
    "        input_ids.append(word2idx.get(token, word2idx.get('<unk>', 3)))\n",
    "\n",
    "    # Truncate or pad to max_len\n",
    "    if len(input_ids) > max_len:\n",
    "        input_ids = input_ids[:max_len]\n",
    "        tokens = khmer_tokens[:max_len]\n",
    "    else:\n",
    "        input_ids += [word2idx.get('<pad>', 1)] * (max_len - len(input_ids))\n",
    "        tokens = khmer_tokens\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = [1 if idx != word2idx.get('<pad>', 1) else 0 for idx in input_ids]\n",
    "\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long)\n",
    "    mask_tensor = torch.tensor([attention_mask], dtype=torch.long)\n",
    "    \n",
    "    return input_tensor, mask_tensor, tokens\n",
    "\n",
    "def fix_bilstm_tags(pred_tags: List[str]) -> List[str]:\n",
    "    \"\"\"Fix invalid BIO sequences by ensuring I- tags follow corresponding B- tags.\"\"\"\n",
    "    fixed_tags = []\n",
    "    for i, tag in enumerate(pred_tags):\n",
    "        if tag.startswith('I-'):\n",
    "            # Check if previous tag is B- or I- of the same entity type\n",
    "            entity_type = tag[2:]  # e.g., 'Disease' from 'I-Disease'\n",
    "            if i == 0 or not (pred_tags[i-1] == f'B-{entity_type}' or pred_tags[i-1] == f'I-{entity_type}'):\n",
    "                logger.warning(f\"Invalid I- tag found at position {i}: {tag}. Converting to B-{entity_type}.\")\n",
    "                fixed_tags.append(f'B-{entity_type}')\n",
    "            else:\n",
    "                fixed_tags.append(tag)\n",
    "        else:\n",
    "            fixed_tags.append(tag)\n",
    "    return fixed_tags\n",
    "\n",
    "def process_bilstm_entities(tokens: List[str], pred_tags: List[str], input_text: str) -> Tuple[List[Tuple], List[Dict]]:\n",
    "    \"\"\"Process BiLSTM-CRF predictions for Gradio output, mapping BIO tags to entity types.\"\"\"\n",
    "    # Fix invalid BIO tags\n",
    "    pred_tags = fix_bilstm_tags(pred_tags)\n",
    "    \n",
    "    highlighted_text = []\n",
    "    processed_entities = []\n",
    "    current_pos = 0\n",
    "    current_entity = []\n",
    "    current_entity_type = None\n",
    "\n",
    "    for token, tag in zip(tokens, pred_tags):\n",
    "        # Map BIO tag to entity type\n",
    "        entity_type = BIO_TO_ENTITY.get(tag, None)\n",
    "        \n",
    "        # Find token in input_text\n",
    "        token_start = input_text.find(token, current_pos)\n",
    "        if token_start == -1:\n",
    "            logger.warning(f\"Token '{token}' not found in input text from position {current_pos}\")\n",
    "            continue\n",
    "        token_end = token_start + len(token)\n",
    "        \n",
    "        # Add non-entity text before token\n",
    "        if token_start > current_pos:\n",
    "            highlighted_text.append((input_text[current_pos:token_start], None))\n",
    "        \n",
    "        # Handle entity aggregation\n",
    "        if tag.startswith('B-'):\n",
    "            if current_entity:  # Save previous entity\n",
    "                entity_text = ''.join(current_entity)\n",
    "                processed_entities.append({\n",
    "                    \"text\": entity_text,\n",
    "                    \"type\": current_entity_type,\n",
    "                    \"score\": 1.0\n",
    "                })\n",
    "                highlighted_text.append((entity_text, current_entity_type))\n",
    "            current_entity = [token]\n",
    "            current_entity_type = entity_type\n",
    "        elif tag.startswith('I-') and entity_type == current_entity_type:\n",
    "            current_entity.append(token)\n",
    "        else:  # O or new entity\n",
    "            if current_entity:\n",
    "                entity_text = ''.join(current_entity)\n",
    "                processed_entities.append({\n",
    "                    \"text\": entity_text,\n",
    "                    \"type\": current_entity_type,\n",
    "                    \"score\": 1.0\n",
    "                })\n",
    "                highlighted_text.append((entity_text, current_entity_type))\n",
    "                current_entity = []\n",
    "                current_entity_type = None\n",
    "            if tag == 'O':\n",
    "                highlighted_text.append((token, None))\n",
    "        \n",
    "        current_pos = token_end\n",
    "    \n",
    "    # Save last entity if exists\n",
    "    if current_entity:\n",
    "        entity_text = ''.join(current_entity)\n",
    "        processed_entities.append({\n",
    "            \"text\": entity_text,\n",
    "            \"type\": current_entity_type,\n",
    "            \"score\": 1.0\n",
    "        })\n",
    "        highlighted_text.append((entity_text, current_entity_type))\n",
    "    \n",
    "    # Add remaining text\n",
    "    if current_pos < len(input_text):\n",
    "        highlighted_text.append((input_text[current_pos:], None))\n",
    "    \n",
    "    return highlighted_text, processed_entities\n",
    "\n",
    "def process_prahokbart_entities(tokens: List[str], pred_tags: List[str], scores: List[float], input_text: str) -> Tuple[List[Tuple], List[Dict]]:\n",
    "    \"\"\"Process PrahokBART predictions for Gradio output.\"\"\"\n",
    "    # Fix invalid BIO tags\n",
    "    pred_tags = fix_bilstm_tags(pred_tags)\n",
    "    \n",
    "    highlighted_text = []\n",
    "    processed_entities = []\n",
    "    current_pos = 0\n",
    "    current_entity = []\n",
    "    current_entity_type = None\n",
    "    current_scores = []\n",
    "\n",
    "    for token, tag, score in zip(tokens, pred_tags, scores):\n",
    "        # Map BIO tag to entity type\n",
    "        entity_type = BIO_TO_ENTITY.get(tag, None)\n",
    "        \n",
    "        # Find token in input_text\n",
    "        token_start = input_text.find(token, current_pos)\n",
    "        if token_start == -1:\n",
    "            # Try to find token with spaces\n",
    "            spaced_token = f\" {token} \"\n",
    "            token_start = input_text.find(spaced_token, current_pos)\n",
    "            if token_start != -1:\n",
    "                token_start += 1  # Skip the leading space\n",
    "                token_end = token_start + len(token)\n",
    "            else:\n",
    "                logger.warning(f\"Token '{token}' not found in input text from position {current_pos}\")\n",
    "                continue\n",
    "        else:\n",
    "            token_end = token_start + len(token)\n",
    "        \n",
    "        # Add non-entity text before token\n",
    "        if token_start > current_pos:\n",
    "            highlighted_text.append((input_text[current_pos:token_start], None))\n",
    "        \n",
    "        # Handle entity aggregation\n",
    "        if tag.startswith('B-'):\n",
    "            if current_entity:  # Save previous entity\n",
    "                entity_text = ''.join(current_entity)\n",
    "                avg_score = np.mean(current_scores) if current_scores else 0.0\n",
    "                processed_entities.append({\n",
    "                    \"text\": entity_text,\n",
    "                    \"type\": current_entity_type,\n",
    "                    \"score\": avg_score\n",
    "                })\n",
    "                highlighted_text.append((entity_text, current_entity_type))\n",
    "            current_entity = [token]\n",
    "            current_entity_type = entity_type\n",
    "            current_scores = [score]\n",
    "        elif tag.startswith('I-') and entity_type == current_entity_type:\n",
    "            current_entity.append(token)\n",
    "            current_scores.append(score)\n",
    "        else:  # O or new entity\n",
    "            if current_entity:\n",
    "                entity_text = ''.join(current_entity)\n",
    "                avg_score = np.mean(current_scores) if current_scores else 0.0\n",
    "                processed_entities.append({\n",
    "                    \"text\": entity_text,\n",
    "                    \"type\": current_entity_type,\n",
    "                    \"score\": avg_score\n",
    "                })\n",
    "                highlighted_text.append((entity_text, current_entity_type))\n",
    "                current_entity = []\n",
    "                current_entity_type = None\n",
    "                current_scores = []\n",
    "            if tag == 'O':\n",
    "                highlighted_text.append((token, None))\n",
    "        \n",
    "        current_pos = token_end\n",
    "    \n",
    "    # Save last entity if exists\n",
    "    if current_entity:\n",
    "        entity_text = ''.join(current_entity)\n",
    "        avg_score = np.mean(current_scores) if current_scores else 0.0\n",
    "        processed_entities.append({\n",
    "            \"text\": entity_text,\n",
    "            \"type\": current_entity_type,\n",
    "            \"score\": avg_score\n",
    "        })\n",
    "        highlighted_text.append((entity_text, current_entity_type))\n",
    "    \n",
    "    # Add remaining text\n",
    "    if current_pos < len(input_text):\n",
    "        highlighted_text.append((input_text[current_pos:], None))\n",
    "    \n",
    "    return highlighted_text, processed_entities\n",
    "\n",
    "def perform_ner(text: str, model_type: str) -> Tuple[gr.HighlightedText, gr.DataFrame]:\n",
    "    \"\"\"\n",
    "    Perform NER on input text using the selected model type.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not text.strip():\n",
    "            logger.warning(\"Empty input text provided\")\n",
    "            return (\n",
    "                [(\"\", None)],\n",
    "                pd.DataFrame({\"Entity\": [], \"Type\": [], \"Confidence\": []})\n",
    "            )\n",
    "        \n",
    "        original_text = text\n",
    "        \n",
    "        if model_type == \"PrahokBART\":\n",
    "            logger.info(\"Using PrahokBART model\")\n",
    "            # Load model and vocabularies\n",
    "            model, tag2idx, idx2tag, word2idx = load_prahokbart_model(PRAHOKBART_MODEL_PATH)\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model.to(device)\n",
    "            \n",
    "            # Preprocess input\n",
    "            input_tensor, mask_tensor, tokens = preprocess_prahokbart_input(text, word2idx)\n",
    "            input_tensor, mask_tensor = input_tensor.to(device), mask_tensor.to(device)\n",
    "            \n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                _, logits = model(input_tensor, mask_tensor)\n",
    "            \n",
    "            # Get predictions\n",
    "            pred_tag_ids = torch.argmax(logits, dim=-1)\n",
    "            scores = torch.softmax(logits, dim=-1).max(dim=-1)[0]\n",
    "            \n",
    "            # Convert predictions to tags\n",
    "            pred_tags = []\n",
    "            confidence_scores = []\n",
    "            mask = mask_tensor[0].cpu().numpy()\n",
    "            \n",
    "            for i, (tag_id, score, m) in enumerate(zip(pred_tag_ids[0], scores[0], mask)):\n",
    "                if m and i < len(tokens):  # Only process non-padded tokens\n",
    "                    tag_idx = int(tag_id.item())\n",
    "                    if 0 <= tag_idx < len(idx2tag):\n",
    "                        pred_tags.append(idx2tag[tag_idx])\n",
    "                    else:\n",
    "                        pred_tags.append('O')\n",
    "                    confidence_scores.append(float(score.item()))\n",
    "            \n",
    "            # Ensure lengths match\n",
    "            min_len = min(len(tokens), len(pred_tags), len(confidence_scores))\n",
    "            tokens = tokens[:min_len]\n",
    "            pred_tags = pred_tags[:min_len]\n",
    "            confidence_scores = confidence_scores[:min_len]\n",
    "            \n",
    "            # Process entities\n",
    "            highlighted_text, processed_entities = process_prahokbart_entities(tokens, pred_tags, confidence_scores, text)\n",
    "            \n",
    "            if not processed_entities:\n",
    "                logger.warning(\"No entities detected by PrahokBART\")\n",
    "                return (\n",
    "                    [(text, None)],\n",
    "                    pd.DataFrame({\"Entity\": [], \"Type\": [], \"Confidence\": []})\n",
    "                )\n",
    "\n",
    "        elif model_type in [\"XLM-RoBERTa\", \"XLM-RoBERTa-Khmer-Small\", \"BERT-Khmer\"]:\n",
    "            logger.info(\"Using %s model\", model_type)\n",
    "            if model_type == \"PrahokBART\":\n",
    "                # Tokenize with khmernltk to match training\n",
    "                tokens = word_tokenize(text)\n",
    "                text = \" \".join(tokens)\n",
    "            \n",
    "            # Load model and tokenizer\n",
    "            model_path = {\n",
    "                \"XLM-RoBERTa\": XLM_MODEL_PATH,\n",
    "                \"XLM-RoBERTa-Khmer-Small\": XLM_FINAL_MODEL_PATH,\n",
    "                \"BERT-Khmer\": BERT_MODEL_PATH,\n",
    "            }[model_type]\n",
    "            model, tokenizer, label2id, id2label = load_model_and_tokenizer(model_path, model_type)\n",
    "            \n",
    "            # Initialize NER pipeline\n",
    "            device = 0 if torch.cuda.is_available() else -1\n",
    "            logger.info(\"Device set to %s\", \"cuda:0\" if device == 0 else \"CPU\")\n",
    "            ner_pipeline = pipeline(\n",
    "                \"ner\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                aggregation_strategy=\"simple\",\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # Run NER pipeline\n",
    "            logger.info(\"Running %s NER pipeline\", model_type)\n",
    "            entities = ner_pipeline(text)\n",
    "            \n",
    "            if not entities:\n",
    "                logger.warning(\"No entities detected by %s\", model_type)\n",
    "                return (\n",
    "                    [(text, None)],\n",
    "                    pd.DataFrame({\"Entity\": [], \"Type\": [], \"Confidence\": []})\n",
    "                )\n",
    "            \n",
    "            # Process entities\n",
    "            highlighted_text, processed_entities = process_entities(entities, text)\n",
    "        \n",
    "        else:  # BiLSTM-CRF\n",
    "            logger.info(\"Using BiLSTM-CRF model\")\n",
    "            # Load BiLSTM-CRF model and vocabularies\n",
    "            model, word_to_idx, tag_to_idx, idx_to_tag = load_bilstm_model_and_vocab()\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model.to(device)\n",
    "            \n",
    "            # Preprocess input\n",
    "            word_tensor, mask_tensor, tokens = preprocess_bilstm_input(text, word_to_idx)\n",
    "            word_tensor, mask_tensor = word_tensor.to(device), mask_tensor.to(device)\n",
    "            \n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                _, pred_tags = model(word_tensor, mask_tensor)\n",
    "            \n",
    "            # Convert predictions to tags\n",
    "            pred_tags = pred_tags[0]\n",
    "            mask = mask_tensor[0].cpu().numpy()\n",
    "            pred_tags = [idx_to_tag.get(int(tag), 'O') for tag, m in zip(pred_tags, mask) if m]\n",
    "            \n",
    "            if len(pred_tags) != len(tokens):\n",
    "                logger.warning(f\"Length mismatch: {len(pred_tags)} tags vs {len(tokens)} tokens\")\n",
    "                prev_len = len(pred_tags)\n",
    "                pred_tags = pred_tags[:len(tokens)]\n",
    "                logger.warning(f\"Truncating from {prev_len} to {len(tokens)}\")\n",
    "            \n",
    "            # Process entities\n",
    "            highlighted_text, processed_entities = process_bilstm_entities(tokens, pred_tags, text)\n",
    "            \n",
    "            if not processed_entities:\n",
    "                logger.warning(\"No entities detected by BiLSTM-CRF\")\n",
    "                return (\n",
    "                    [(text, None)],\n",
    "                    pd.DataFrame({\"Entity\": [], \"Type\": [], \"Confidence\": []})\n",
    "                )\n",
    "\n",
    "        # Prepare DataFrame\n",
    "        df_data = [{\n",
    "            \"Entity\": entity['text'],\n",
    "            \"Type\": entity['type'],\n",
    "            \"Confidence\": f\"{entity['score']:.4f}\"\n",
    "        } for entity in processed_entities]\n",
    "        \n",
    "        df = pd.DataFrame(df_data)\n",
    "        \n",
    "        logger.info(\"Processed %d entities\", len(processed_entities))\n",
    "        return (\n",
    "            highlighted_text,\n",
    "            df\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error during NER processing: %s\", str(e))\n",
    "        return (\n",
    "            [(text, None)],\n",
    "            pd.DataFrame({\"Entity\": [], \"Type\": [], \"Confidence\": []})\n",
    "        )\n",
    "\n",
    "def load_model_and_tokenizer(model_path: str, model_type: str) -> Tuple[Union[XLMRobertaForTokenClassification, BertForTokenClassification], Union[XLMRobertaTokenizerFast, BertTokenizerFast, MBartTokenizerFast], Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Load the fine-tuned model, tokenizer, and label mappings based on model type.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model_type in [\"XLM-RoBERTa\", \"XLM-RoBERTa-Khmer-Small\"]:\n",
    "            from transformers import XLMRobertaForTokenClassification, XLMRobertaTokenizerFast\n",
    "            logger.info(\"Loading XLM-RoBERTa tokenizer from %s\", model_path)\n",
    "            tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_path)\n",
    "            logger.info(\"Loading XLM-RoBERTa model from %s\", model_path)\n",
    "            model = XLMRobertaForTokenClassification.from_pretrained(model_path)\n",
    "        elif model_type == \"BERT-Khmer\":\n",
    "            from transformers import BertForTokenClassification, BertTokenizerFast\n",
    "            logger.info(\"Loading BERT-Khmer tokenizer from %s\", model_path)\n",
    "            tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "            logger.info(\"Loading BERT-Khmer model from %s\", model_path)\n",
    "            model = BertForTokenClassification.from_pretrained(model_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "        logger.info(\"Tokenizer and model loaded successfully\")\n",
    "\n",
    "        # Load label mappings\n",
    "        label2id_path = os.path.join(model_path, \"label2id.json\") if model_type == \"BERT-Khmer\" else LABEL2ID_PATH\n",
    "        logger.info(\"Loading label mappings from %s\", label2id_path)\n",
    "        with open(label2id_path, 'r', encoding='utf-8') as f:\n",
    "            label2id = json.load(f)\n",
    "        id2label = {int(k): v for k, v in model.config.id2label.items()}\n",
    "        logger.info(\"Label mappings loaded successfully\")\n",
    "\n",
    "        return model, tokenizer, label2id, id2label\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error loading model/tokenizer/label mappings: %s\", str(e))\n",
    "        raise\n",
    "\n",
    "def process_entities(entities: List[Dict], input_text: str) -> Tuple[List[Tuple], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Process raw XLM-RoBERTa NER pipeline output for Gradio.\n",
    "    \"\"\"\n",
    "    processed_entities = []\n",
    "    highlighted_text = []\n",
    "    last_end = 0\n",
    "\n",
    "    for entity in entities:\n",
    "        entity_type = entity['entity_group']\n",
    "        score = float(entity['score'])\n",
    "        word = entity['word']\n",
    "        start = entity['start']\n",
    "        end = entity['end']\n",
    "\n",
    "        if start > last_end:\n",
    "            highlighted_text.append((input_text[last_end:start], None))\n",
    "\n",
    "        highlighted_text.append((input_text[start:end], entity_type))\n",
    "\n",
    "        processed_entities.append({\n",
    "            \"text\": word,\n",
    "            \"type\": entity_type,\n",
    "            \"score\": score\n",
    "        })\n",
    "\n",
    "        last_end = end\n",
    "\n",
    "    if last_end < len(input_text):\n",
    "        highlighted_text.append((input_text[last_end:], None))\n",
    "\n",
    "    return highlighted_text, processed_entities\n",
    "\n",
    "# Define default example text\n",
    "DEFAULT_TEXT = \"\"\"\n",
    "នៅខេត្តសៀមរាប៖ ជំងឺរលាកសួតបានរកឃើញនៅថ្ងៃទី៥ ខែមករា ឆ្នាំ២០២៥ ដោយមានអ្នកជំងឺ៥៦នាក់ ប្រើថ្នាំអាស្ពីរីន និងមានរោគសញ្ញាក្អក និងគ្រុនក្តៅ បណ្តាលមកពីមេរោគអេសអិន១១។\n",
    "\"\"\"\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"Khmer NER Interface\") as demo:\n",
    "    gr.Markdown(\"# Khmer Named Entity Recognition (NER)\")\n",
    "    gr.Markdown(\"Enter Khmer text and select a model to identify entities such as Disease, Location, Date, etc.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        input_text = gr.Textbox(\n",
    "            label=\"Input Text\",\n",
    "            value=DEFAULT_TEXT,\n",
    "            lines=5,\n",
    "            placeholder=\"Enter Khmer text here...\"\n",
    "        )\n",
    "        model_choice = gr.Dropdown(\n",
    "            label=\"Model Type\",\n",
    "            choices=[\"XLM-RoBERTa\", \"XLM-RoBERTa-Khmer-Small\", \"BERT-Khmer\", \"PrahokBART\", \"BiLSTM-CRF\"],\n",
    "            value=\"XLM-RoBERTa\"\n",
    "        )\n",
    "    \n",
    "    submit_button = gr.Button(\"Perform NER\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        highlighted_output = gr.HighlightedText(\n",
    "            label=\"Highlighted Entities\",\n",
    "            show_legend=True,\n",
    "            color_map=ENTITY_COLORS\n",
    "        )\n",
    "        entity_table = gr.DataFrame(\n",
    "            label=\"Detected Entities\",\n",
    "            headers=[\"Entity\", \"Type\", \"Confidence\"]\n",
    "        )\n",
    "    \n",
    "    submit_button.click(\n",
    "        fn=perform_ner,\n",
    "        inputs=[input_text, model_choice],\n",
    "        outputs=[highlighted_output, entity_table]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(\"Launching Gradio interface\")\n",
    "        demo.launch(share=True)  # Set share=False for local testing\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to launch Gradio interface: %s\", str(e))\n",
    "        print(f\"Error launching interface: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kheed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
