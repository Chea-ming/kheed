{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7b9630",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598de6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/Public/kheed/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorForTokenClassification\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    XLMRobertaForTokenClassification, \n",
    "    XLMRobertaTokenizerFast, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "\n",
    "from seqeval.metrics import (\n",
    "    precision_score as seq_precision,\n",
    "    recall_score as seq_recall,\n",
    "    f1_score as seq_f1,\n",
    "    accuracy_score,\n",
    "    classification_report as seq_classification_report\n",
    ")\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", message=\"Some weights of.*were not initialized from the model checkpoint.*\")\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae288400",
   "metadata": {},
   "source": [
    "# Analyze Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "685c1529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Analyzing data structure...\n",
      "ğŸ“ Found 525 JSON files\n",
      "ğŸ“‹ Analyzing first 5 files...\n",
      "\n",
      "File 1: object_d2279a49-8b25-4e4c-b936-62f88487a895.json\n",
      "  ğŸ“ Keys: ['id', 'annotations', 'file_upload', 'drafts', 'predictions', 'data', 'meta', 'created_at', 'updated_at', 'inner_id', 'total_annotations', 'cancelled_annotations', 'total_predictions', 'comment_count', 'unresolved_comment_count', 'last_comment_updated_at', 'project', 'updated_by', 'comment_authors', 'processed_content']\n",
      "  ğŸ“Š Sentences in file: 9\n",
      "    Sentence 1:\n",
      "      ğŸ”¤ Tokens (81): ['á§á”', 'á“á¶á™á€', 'ášáŠáŸ’á‹á˜á“áŸ’ááŸ’ášá¸', ' ', 'á“áŸá']...\n",
      "      ğŸ·ï¸  Tags (81): [['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "    Sentence 2:\n",
      "      ğŸ”¤ Tokens (83): ['ááŸ’á›áŸ‚á„', 'á€áŸ’á“á»á„', 'á–á·á’á¸', 'á”áŸ’ášá€á¶áŸ', 'á‡á¶']...\n",
      "      ğŸ·ï¸  Tags (83): [['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "\n",
      "File 2: object_e1a96cb5-7830-4846-a601-0a0357497b28.json\n",
      "  ğŸ“ Keys: ['id', 'annotations', 'file_upload', 'drafts', 'predictions', 'data', 'meta', 'created_at', 'updated_at', 'inner_id', 'total_annotations', 'cancelled_annotations', 'total_predictions', 'comment_count', 'unresolved_comment_count', 'last_comment_updated_at', 'project', 'updated_by', 'comment_authors', 'processed_content']\n",
      "  ğŸ“Š Sentences in file: 32\n",
      "    Sentence 1:\n",
      "      ğŸ”¤ Tokens (43): ['á€áŸ’ášáŸá½á„', 'áŸá»áá¶á—á·á”á¶á›', 'á“á·á„', 'áŠáŸƒá‚á¼', 'á–á¶á€áŸ‹á–áŸá“áŸ’á’']...\n",
      "      ğŸ·ï¸  Tags (43): [['B-Organization'], ['I-Organization'], ['O'], ['O'], ['O']]...\n",
      "    Sentence 2:\n",
      "      ğŸ”¤ Tokens (34): ['á™á»á‘áŸ’á’á“á¶á€á¶áš', 'á“áŸáŸ‡', 'á‚áºá‡á¶', 'á€á¶ášá†áŸ’á›á¾á™áá”', 'á‘á¶á“áŸ‹']...\n",
      "      ğŸ·ï¸  Tags (34): [['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "\n",
      "File 3: object_792495e7-39e6-4902-90b7-5edecd04877b.json\n",
      "  ğŸ“ Keys: ['id', 'annotations', 'file_upload', 'drafts', 'predictions', 'data', 'meta', 'created_at', 'updated_at', 'inner_id', 'total_annotations', 'cancelled_annotations', 'total_predictions', 'comment_count', 'unresolved_comment_count', 'last_comment_updated_at', 'project', 'updated_by', 'comment_authors', 'processed_content']\n",
      "  ğŸ“Š Sentences in file: 11\n",
      "    Sentence 1:\n",
      "      ğŸ”¤ Tokens (60): ['á›áŸ„á€', ' ', 'á‚á¸á˜', ' ', 'ášá·á‘áŸ’á’á¸']...\n",
      "      ğŸ·ï¸  Tags (60): [['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "    Sentence 2:\n",
      "      ğŸ”¤ Tokens (88): ['á€á¶ášááŸ’á›áŸ‚á„', 'ášá”áŸáŸ‹', 'á›áŸ„á€', ' ', 'á‚á¸á˜']...\n",
      "      ğŸ·ï¸  Tags (88): [['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "\n",
      "File 4: object_1fe657cc-df5b-4309-aaf2-e25e485b8ff4.json\n",
      "  ğŸ“ Keys: ['id', 'annotations', 'file_upload', 'drafts', 'predictions', 'data', 'meta', 'created_at', 'updated_at', 'inner_id', 'total_annotations', 'cancelled_annotations', 'total_predictions', 'comment_count', 'unresolved_comment_count', 'last_comment_updated_at', 'project', 'updated_by', 'comment_authors', 'processed_content']\n",
      "  ğŸ“Š Sentences in file: 10\n",
      "    Sentence 1:\n",
      "      ğŸ”¤ Tokens (44): ['á›áŸ„á€', 'áŸá¶áŸáŸ’áŠáŸ’ášá¶á…á¶ášáŸ’á™', ' ', 'áˆá¶á„', ' ']...\n",
      "      ğŸ·ï¸  Tags (44): [['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "    Sentence 2:\n",
      "      ğŸ”¤ Tokens (45): ['á€áŸ’á“á»á„', 'á“áŸ„áŸ‡', 'á€á¶ášá–á„áŸ’ášá¹á„', 'áŸáŸ†áŠáŸ…', 'á›á¾']...\n",
      "      ğŸ·ï¸  Tags (45): [['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "\n",
      "File 5: object_04d28d27-96e9-4d95-b946-c1475b2207d2.json\n",
      "  ğŸ“ Keys: ['id', 'annotations', 'file_upload', 'drafts', 'predictions', 'data', 'meta', 'created_at', 'updated_at', 'inner_id', 'total_annotations', 'cancelled_annotations', 'total_predictions', 'comment_count', 'unresolved_comment_count', 'last_comment_updated_at', 'project', 'updated_by', 'comment_authors', 'processed_content']\n",
      "  ğŸ“Š Sentences in file: 5\n",
      "    Sentence 1:\n",
      "      ğŸ”¤ Tokens (64): ['á€á¶ášá›á¾á€á¡á¾á„', 'ášá”áŸáŸ‹', ' ', 'á›áŸ„á€', ' ']...\n",
      "      ğŸ·ï¸  Tags (64): [['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "    Sentence 2:\n",
      "      ğŸ”¤ Tokens (47): ['á›áŸ„á€', 'á¢á‚áŸ’á‚', 'á“á¶á™á€', ' ', 'á”á¶á“']...\n",
      "      ğŸ·ï¸  Tags (47): [['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "\n",
      "ğŸ·ï¸  Unique tags found: ['B-Date', 'B-Disease', 'B-HumanCount', 'B-Location', 'B-Medication', 'B-Organization', 'I-Date', 'I-Disease', 'I-HumanCount', 'I-Location', 'I-Medication', 'I-Organization', 'O']\n",
      "ğŸ“ˆ Total unique tags: 13\n"
     ]
    }
   ],
   "source": [
    "def analyze_data_structure(input_dir, max_files_to_check=5):\n",
    "    \"\"\"\n",
    "    Analyze the structure of JSON files to understand the data format.\n",
    "    \"\"\"\n",
    "    input_files = glob(os.path.join(input_dir, \"*.json\"))\n",
    "    \n",
    "    if not input_files:\n",
    "        print(f\"âŒ No JSON files found in '{input_dir}'\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ“ Found {len(input_files)} JSON files\")\n",
    "    print(f\"ğŸ“‹ Analyzing first {min(max_files_to_check, len(input_files))} files...\\n\")\n",
    "    \n",
    "    all_tags = set()\n",
    "    sample_data = []\n",
    "    \n",
    "    for i, file_path in enumerate(input_files[:max_files_to_check]):\n",
    "        print(f\"File {i+1}: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Show file structure\n",
    "            print(f\"  ğŸ“ Keys: {list(data.keys())}\")\n",
    "            \n",
    "            if 'processed_content' in data:\n",
    "                processed_content = data['processed_content']\n",
    "                print(f\"  ğŸ“Š Sentences in file: {len(processed_content)}\")\n",
    "                \n",
    "                # Check first few sentences\n",
    "                for j, sentence in enumerate(processed_content[:2]):\n",
    "                    tokens = sentence.get('tokens', [])\n",
    "                    bio_tags = sentence.get('bio_tags', [])\n",
    "                    \n",
    "                    print(f\"    Sentence {j+1}:\")\n",
    "                    print(f\"      ğŸ”¤ Tokens ({len(tokens)}): {tokens[:5]}{'...' if len(tokens) > 5 else ''}\")\n",
    "                    print(f\"      ğŸ·ï¸  Tags ({len(bio_tags)}): {bio_tags[:5]}{'...' if len(bio_tags) > 5 else ''}\")\n",
    "                    \n",
    "                    # Flatten tags and collect unique ones\n",
    "                    flat_tags = [tag[0] if isinstance(tag, list) and tag else tag for tag in bio_tags]\n",
    "                    all_tags.update(flat_tags)\n",
    "                    \n",
    "                    if j == 0:  # Save first sentence as sample\n",
    "                        sample_data.append({\n",
    "                            'file': os.path.basename(file_path),\n",
    "                            'tokens': tokens,\n",
    "                            'bio_tags': flat_tags\n",
    "                        })\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error reading file: {e}\\n\")\n",
    "    \n",
    "    print(f\"ğŸ·ï¸  Unique tags found: {sorted(all_tags)}\")\n",
    "    print(f\"ğŸ“ˆ Total unique tags: {len(all_tags)}\")\n",
    "    \n",
    "    return all_tags, sample_data\n",
    "\n",
    "# %%\n",
    "# Analyze your data structure\n",
    "input_dir = \"/home/guest/Public/KHEED/KHEED_Data_Collection/Final/bio_tagged\"\n",
    "output_dir = \"/home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/xlmr_ner_model\"\n",
    "\n",
    "print(\"ğŸ” Analyzing data structure...\")\n",
    "unique_tags, sample_data = analyze_data_structure(input_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e264f",
   "metadata": {},
   "source": [
    "# Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e3ba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading data...\n",
      "ğŸ“‚ Processing 525 files...\n",
      "  ğŸ“„ Processing: object_d2279a49-8b25-4e4c-b936-62f88487a895.json\n",
      "  ğŸ“„ Processing: object_e1a96cb5-7830-4846-a601-0a0357497b28.json\n",
      "  ğŸ“„ Processing: object_792495e7-39e6-4902-90b7-5edecd04877b.json\n",
      "  ğŸ“„ Processing: object_1fe657cc-df5b-4309-aaf2-e25e485b8ff4.json\n",
      "  ğŸ“„ Processing: object_04d28d27-96e9-4d95-b946-c1475b2207d2.json\n",
      "\n",
      "ğŸ“Š Data loading summary:\n",
      "  âœ… Processed sentences: 6221\n",
      "  âš ï¸  Skipped sentences: 12\n",
      "  âŒ Total errors: 0\n",
      "âœ… Successfully loaded 6221 sentences from 525 files\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def load_json_files_robust(input_dir, verbose=True):\n",
    "    \"\"\"\n",
    "    Load JSON files and extract tokens and BIO tags with robust error handling.\n",
    "    \"\"\"\n",
    "    all_tokens = []\n",
    "    all_tags = []\n",
    "    input_files = glob(os.path.join(input_dir, \"*.json\"))\n",
    "    \n",
    "    if not input_files:\n",
    "        print(f\"âŒ No JSON files found in '{input_dir}'. Please check the directory.\")\n",
    "        return [], [], []\n",
    "    \n",
    "    print(f\"ğŸ“‚ Processing {len(input_files)} files...\")\n",
    "    \n",
    "    errors = []\n",
    "    skipped_sentences = 0\n",
    "    processed_sentences = 0\n",
    "    \n",
    "    for file_idx, input_file in enumerate(input_files):\n",
    "        if verbose and file_idx < 5:\n",
    "            print(f\"  ğŸ“„ Processing: {os.path.basename(input_file)}\")\n",
    "        \n",
    "        try:\n",
    "            with open(input_file, 'r', encoding='utf-8') as f:\n",
    "                obj = json.load(f)\n",
    "            \n",
    "            processed_content = obj.get('processed_content', [])\n",
    "            \n",
    "            for sent_idx, sentence_data in enumerate(processed_content):\n",
    "                try:\n",
    "                    tokens = sentence_data.get('tokens', [])\n",
    "                    bio_tags = sentence_data.get('bio_tags', [])\n",
    "                    \n",
    "                    if not tokens or not bio_tags:\n",
    "                        skipped_sentences += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Handle nested tag structure\n",
    "                    flattened_tags = []\n",
    "                    for tag in bio_tags:\n",
    "                        if isinstance(tag, list):\n",
    "                            # Take first element if it's a list\n",
    "                            flattened_tags.append(tag[0] if tag else \"O\")\n",
    "                        else:\n",
    "                            flattened_tags.append(tag if tag else \"O\")\n",
    "                    \n",
    "                    # Check length mismatch\n",
    "                    if len(tokens) != len(flattened_tags):\n",
    "                        error_msg = f\"Length mismatch in {os.path.basename(input_file)}, sentence {sent_idx}: {len(tokens)} tokens vs {len(flattened_tags)} tags\"\n",
    "                        errors.append(error_msg)\n",
    "                        if verbose and len(errors) <= 3:\n",
    "                            print(f\"    âš ï¸  {error_msg}\")\n",
    "                        skipped_sentences += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Validate tags - replace unknown tags with 'O'\n",
    "                    validated_tags = []\n",
    "                    for tag in flattened_tags:\n",
    "                        if tag and isinstance(tag, str):\n",
    "                            validated_tags.append(tag)\n",
    "                        else:\n",
    "                            validated_tags.append(\"O\")\n",
    "                    \n",
    "                    all_tokens.append(tokens)\n",
    "                    all_tags.append(validated_tags)\n",
    "                    processed_sentences += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error processing sentence {sent_idx} in {os.path.basename(input_file)}: {e}\"\n",
    "                    errors.append(error_msg)\n",
    "                    if verbose and len(errors) <= 3:\n",
    "                        print(f\"    âŒ {error_msg}\")\n",
    "                    skipped_sentences += 1\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error reading file {input_file}: {e}\"\n",
    "            errors.append(error_msg)\n",
    "            if verbose:\n",
    "                print(f\"  âŒ {error_msg}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Data loading summary:\")\n",
    "    print(f\"  âœ… Processed sentences: {processed_sentences}\")\n",
    "    print(f\"  âš ï¸  Skipped sentences: {skipped_sentences}\")\n",
    "    print(f\"  âŒ Total errors: {len(errors)}\")\n",
    "    \n",
    "    if errors and verbose:\n",
    "        print(f\"\\nğŸ” First few errors:\")\n",
    "        for error in errors[:5]:\n",
    "            print(f\"    â€¢ {error}\")\n",
    "    \n",
    "    return all_tokens, all_tags, input_files\n",
    "\n",
    "# %%\n",
    "# Load data with robust error handling\n",
    "print(\"ğŸ“¥ Loading data...\")\n",
    "all_tokens, all_tags, input_files = load_json_files_robust(input_dir)\n",
    "\n",
    "if not all_tokens:\n",
    "    print(\"âŒ No data loaded. Please check your input directory and file format.\")\n",
    "else:\n",
    "    print(f\"âœ… Successfully loaded {len(all_tokens)} sentences from {len(input_files)} files\")\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339772d0",
   "metadata": {},
   "source": [
    "# Analyze tag distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4abceab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ·ï¸  Analyzing tag distribution...\n",
      "ğŸ“ˆ Tag statistics:\n",
      "  Total tokens: 251,292\n",
      "  Unique tags: 17\n",
      "\n",
      "ğŸ”¢ Tag distribution:\n",
      "  O               | 217,651 (86.61%)\n",
      "  I-Organization  |  7,133 ( 2.84%)\n",
      "  I-Disease       |  4,526 ( 1.80%)\n",
      "  I-Date          |  4,206 ( 1.67%)\n",
      "  B-Disease       |  3,167 ( 1.26%)\n",
      "  B-Organization  |  3,067 ( 1.22%)\n",
      "  B-Location      |  2,858 ( 1.14%)\n",
      "  I-Location      |  2,618 ( 1.04%)\n",
      "  B-Date          |  1,360 ( 0.54%)\n",
      "  I-HumanCount    |  1,186 ( 0.47%)\n",
      "  B-HumanCount    |    855 ( 0.34%)\n",
      "  I-Pathogen      |    580 ( 0.23%)\n",
      "  I-Medication    |    536 ( 0.21%)\n",
      "  B-Pathogen      |    490 ( 0.19%)\n",
      "  B-Symptom       |    468 ( 0.19%)\n",
      "  I-Symptom       |    375 ( 0.15%)\n",
      "  B-Medication    |    216 ( 0.09%)\n",
      "\n",
      "ğŸ” Data quality checks:\n",
      "  âœ… No rare tags found\n",
      "ğŸ¯ Setting up model and data...\n",
      "ğŸ·ï¸  Label mapping created:\n",
      "  Total labels: 17\n",
      "  Labels: ['B-Date', 'B-Disease', 'B-HumanCount', 'B-Location', 'B-Medication', 'B-Organization', 'B-Pathogen', 'B-Symptom', 'I-Date', 'I-Disease', 'I-HumanCount', 'I-Location', 'I-Medication', 'I-Organization', 'I-Pathogen', 'I-Symptom', 'O']\n",
      "ğŸ“Š Data split:\n",
      "  Training: 4,976 sentences\n",
      "  Validation: 622 sentences\n",
      "  Test: 623 sentences\n",
      "ğŸ¤– Loading model: seanghay/xlm-roberta-khmer-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at seanghay/xlm-roberta-khmer-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model and tokenizer loaded successfully\n",
      "ğŸ”„ Preparing datasets...\n",
      "ğŸ”„ Preparing dataset...\n",
      "âœ… Prepared 4976 examples, skipped 0\n",
      "ğŸ”„ Preparing dataset...\n",
      "âœ… Prepared 622 examples, skipped 0\n",
      "ğŸ”„ Preparing dataset...\n",
      "âœ… Prepared 623 examples, skipped 0\n",
      "ğŸ’¾ Saved mappings to /home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/xlmr_ner_model\n"
     ]
    }
   ],
   "source": [
    "if all_tags:\n",
    "    print(\"ğŸ·ï¸  Analyzing tag distribution...\")\n",
    "    \n",
    "    # Flatten all tags\n",
    "    flat_tags = [tag for sentence_tags in all_tags for tag in sentence_tags]\n",
    "    tag_counts = Counter(flat_tags)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Tag statistics:\")\n",
    "    print(f\"  Total tokens: {len(flat_tags):,}\")\n",
    "    print(f\"  Unique tags: {len(tag_counts)}\")\n",
    "    \n",
    "    # Show tag distribution\n",
    "    print(f\"\\nğŸ”¢ Tag distribution:\")\n",
    "    for tag, count in tag_counts.most_common():\n",
    "        percentage = (count / len(flat_tags)) * 100\n",
    "        print(f\"  {tag:15} | {count:6,} ({percentage:5.2f}%)\")\n",
    "    \n",
    "    # Check for potential issues\n",
    "    print(f\"\\nğŸ” Data quality checks:\")\n",
    "    rare_tags = [tag for tag, count in tag_counts.items() if count < 10]\n",
    "    if rare_tags:\n",
    "        print(f\"  âš ï¸  Rare tags (< 10 occurrences): {rare_tags}\")\n",
    "    else:\n",
    "        print(f\"  âœ… No rare tags found\")\n",
    "\n",
    "# %%\n",
    "def prepare_dataset_robust(tokens, tags, tokenizer, label2id, max_length=512):\n",
    "    \"\"\"\n",
    "    Prepare dataset for XLM-RoBERTa with robust error handling.\n",
    "    \"\"\"\n",
    "    encodings = []\n",
    "    skipped = 0\n",
    "    \n",
    "    print(f\"ğŸ”„ Preparing dataset...\")\n",
    "    \n",
    "    for i, (token_list, tag_list) in enumerate(zip(tokens, tags)):\n",
    "        try:\n",
    "            # Validate inputs\n",
    "            if not token_list or not tag_list or len(token_list) != len(tag_list):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Check if all tags exist in label2id\n",
    "            missing_tags = [tag for tag in tag_list if tag not in label2id]\n",
    "            if missing_tags:\n",
    "                print(f\"âš ï¸  Sentence {i}: Missing tags {set(missing_tags)} - replacing with 'O'\")\n",
    "                tag_list = [tag if tag in label2id else 'O' for tag in tag_list]\n",
    "            \n",
    "            encoding = tokenizer(\n",
    "                token_list,\n",
    "                is_split_into_words=True,\n",
    "                return_offsets_mapping=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=False  # Will be padded by data collator\n",
    "            )\n",
    "            \n",
    "            aligned_labels = []\n",
    "            word_ids = encoding.word_ids()\n",
    "            prev_word_id = None\n",
    "            \n",
    "            for word_id in word_ids:\n",
    "                if word_id is None:\n",
    "                    # Special tokens get -100\n",
    "                    aligned_labels.append(-100)\n",
    "                elif word_id != prev_word_id:\n",
    "                    # First subword of a word gets the actual label\n",
    "                    if word_id < len(tag_list):\n",
    "                        aligned_labels.append(label2id[tag_list[word_id]])\n",
    "                    else:\n",
    "                        aligned_labels.append(label2id['O'])\n",
    "                else:\n",
    "                    # Subsequent subwords of the same word\n",
    "                    if word_id < len(tag_list):\n",
    "                        tag = tag_list[word_id]\n",
    "                        if tag.startswith(\"B-\"):\n",
    "                            # Convert B- to I- for subwords\n",
    "                            i_tag = f\"I-{tag[2:]}\"\n",
    "                            aligned_labels.append(label2id.get(i_tag, label2id[tag]))\n",
    "                        else:\n",
    "                            aligned_labels.append(label2id[tag])\n",
    "                    else:\n",
    "                        aligned_labels.append(label2id['O'])\n",
    "                \n",
    "                prev_word_id = word_id\n",
    "            \n",
    "            encoding[\"labels\"] = aligned_labels\n",
    "            del encoding[\"offset_mapping\"]  # Remove offset mapping to save memory\n",
    "            encodings.append(encoding)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing sentence {i}: {e}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"âœ… Prepared {len(encodings)} examples, skipped {skipped}\")\n",
    "    return encodings\n",
    "\n",
    "# %%\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom Dataset for NER.\"\"\"\n",
    "    \n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "# %%\n",
    "def setup_model_and_data(all_tokens, all_tags, model_name=\"seanghay/xlm-roberta-khmer-small\", test_size=0.2, val_size=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Set up model, tokenizer, and datasets.\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    print(f\"ğŸ¯ Setting up model and data...\")\n",
    "    \n",
    "    # Get unique labels and create mappings\n",
    "    unique_labels = set()\n",
    "    for tags in all_tags:\n",
    "        unique_labels.update(tags)\n",
    "    \n",
    "    # Ensure 'O' tag exists\n",
    "    unique_labels.add('O')\n",
    "    \n",
    "    # Create label mappings\n",
    "    sorted_labels = sorted(unique_labels)\n",
    "    label2id = {label: idx for idx, label in enumerate(sorted_labels)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "    \n",
    "    print(f\"ğŸ·ï¸  Label mapping created:\")\n",
    "    print(f\"  Total labels: {len(label2id)}\")\n",
    "    print(f\"  Labels: {sorted_labels}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_tokens, temp_tokens, train_tags, temp_tags = train_test_split(\n",
    "        all_tokens, all_tags, test_size=test_size, random_state=seed, stratify=None\n",
    "    )\n",
    "    \n",
    "    val_tokens, test_tokens, val_tags, test_tags = train_test_split(\n",
    "        temp_tokens, temp_tags, test_size=0.5, random_state=seed, stratify=None\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“Š Data split:\")\n",
    "    print(f\"  Training: {len(train_tokens):,} sentences\")\n",
    "    print(f\"  Validation: {len(val_tokens):,} sentences\") \n",
    "    print(f\"  Test: {len(test_tokens):,} sentences\")\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    print(f\"ğŸ¤– Loading model: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_name)\n",
    "        model = XLMRobertaForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(label2id),\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True  # Handle size mismatches\n",
    "        )\n",
    "        print(\"âœ… Model and tokenizer loaded successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading model: {e}\")\n",
    "        print(\"ğŸ”„ Falling back to xlm-roberta-base...\")\n",
    "        tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
    "        model = XLMRobertaForTokenClassification.from_pretrained(\n",
    "            \"xlm-roberta-base\",\n",
    "            num_labels=len(label2id),\n",
    "            id2label=id2label,\n",
    "            label2id=label2id\n",
    "        )\n",
    "    \n",
    "    # Prepare datasets\n",
    "    print(\"ğŸ”„ Preparing datasets...\")\n",
    "    train_encodings = prepare_dataset_robust(train_tokens, train_tags, tokenizer, label2id)\n",
    "    val_encodings = prepare_dataset_robust(val_tokens, val_tags, tokenizer, label2id)\n",
    "    test_encodings = prepare_dataset_robust(test_tokens, test_tags, tokenizer, label2id)\n",
    "    \n",
    "    train_dataset = NERDataset(train_encodings)\n",
    "    val_dataset = NERDataset(val_encodings)\n",
    "    test_dataset = NERDataset(test_encodings)\n",
    "    \n",
    "    return model, tokenizer, train_dataset, val_dataset, test_dataset, label2id, id2label\n",
    "\n",
    "# %%\n",
    "# Set up model and datasets\n",
    "if all_tokens and all_tags:\n",
    "    model, tokenizer, train_dataset, val_dataset, test_dataset, label2id, id2label = setup_model_and_data(\n",
    "        all_tokens, all_tags\n",
    "    )\n",
    "    \n",
    "    # Save mappings\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"label2id.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(label2id, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"input_files.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(input_files, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"ğŸ’¾ Saved mappings to {output_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0440c21c",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adb4a7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Starting manual model training...\n",
      "ğŸ–¥ï¸  Using device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "   Memory: 16.9 GB\n",
      "ğŸš€ Starting training for Khmer NER...\n",
      "   Epochs: 6\n",
      "   Per-device batch size: 8\n",
      "   Gradient accumulation steps: 2\n",
      "   Effective batch size: 16\n",
      "   Learning rate: 2e-05\n",
      "   Early stopping patience: 2 epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1555' max='1866' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1555/1866 00:45 < 00:09, 33.98 it/s, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.526900</td>\n",
       "      <td>0.158125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.137400</td>\n",
       "      <td>0.148432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.136962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.091800</td>\n",
       "      <td>0.145450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.148990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training completed!\n",
      "ğŸ’¾ Final model saved to /home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/xlmr_ner_model/final_model\n",
      "ğŸ“Š Starting manual model evaluation...\n",
      "ğŸ” Evaluating model on test set...\n",
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:00<00:00, 247.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Evaluation completed on 623 samples\n",
      "ğŸ’¾ Detailed predictions saved to /home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/xlmr_ner_model/predictions.json\n",
      "ğŸ’¾ CoNLL format saved to /home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/xlmr_ner_model/predictions.conll\n",
      "\n",
      "ğŸ“Š Comprehensive NER Evaluation Results:\n",
      "================================================================================\n",
      "ğŸ¯ ENTITY-LEVEL METRICS (Primary - Standard for NER)\n",
      "--------------------------------------------------\n",
      "Entity Precision: 0.5943\n",
      "Entity Recall:    0.7793\n",
      "Entity F1-Score:  0.6744\n",
      "Entity Accuracy:  0.9505\n",
      "\n",
      "ğŸ“‹ Entity-level Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Disease       0.67      0.82      0.74       287\n",
      "    Location       0.66      0.82      0.73       260\n",
      "        Date       0.83      0.87      0.85       154\n",
      "  HumanCount       0.65      0.85      0.74        91\n",
      "Organization       0.45      0.64      0.53       270\n",
      "    Pathogen       0.55      0.74      0.63        46\n",
      "     Symptom       0.37      0.78      0.51        32\n",
      "  Medication       0.20      0.55      0.30        20\n",
      "\n",
      "   micro avg       0.59      0.78      0.67      1160\n",
      "   macro avg       0.62      0.78      0.68      1160\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ”¤ TOKEN-LEVEL METRICS (Secondary)\n",
      "--------------------------------------------------\n",
      "Token Accuracy: 0.9505\n",
      "Token Precision (Micro): 0.9505\n",
      "Token Recall (Micro):    0.9505\n",
      "Token F1-Score (Micro):  0.9505\n",
      "Token Precision (Macro): 0.7410\n",
      "Token Recall (Macro):    0.8533\n",
      "Token F1-Score (Macro):  0.7854\n",
      "\n",
      "ğŸ“‹ Token-level Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        B-Date       0.93      0.93      0.93       153\n",
      "     B-Disease       0.85      0.90      0.87       286\n",
      "  B-HumanCount       0.70      0.88      0.78        86\n",
      "    B-Location       0.81      0.90      0.85       260\n",
      "  B-Medication       0.52      0.60      0.56        20\n",
      "B-Organization       0.77      0.83      0.80       268\n",
      "    B-Pathogen       0.71      0.78      0.74        46\n",
      "     B-Symptom       0.47      0.88      0.61        32\n",
      "        I-Date       0.95      0.95      0.95       637\n",
      "     I-Disease       0.85      0.90      0.88      1504\n",
      "  I-HumanCount       0.87      0.90      0.88       327\n",
      "    I-Location       0.82      0.89      0.86       752\n",
      "  I-Medication       0.48      0.66      0.56       155\n",
      "I-Organization       0.79      0.76      0.78      1625\n",
      "    I-Pathogen       0.70      0.87      0.77       255\n",
      "     I-Symptom       0.41      0.90      0.56       111\n",
      "             O       0.98      0.97      0.98     33858\n",
      "\n",
      "      accuracy                           0.95     40375\n",
      "     macro avg       0.74      0.85      0.79     40375\n",
      "  weighted avg       0.95      0.95      0.95     40375\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ ADDITIONAL ANALYSIS\n",
      "--------------------------------------------------\n",
      "Total tokens: 40,375\n",
      "Entity tokens: 6,517 (16.1%)\n",
      "O tokens: 33,858 (83.9%)\n",
      "Total sequences: 623\n",
      "\n",
      "ğŸ·ï¸ Entity Type Distribution:\n",
      "  I-Organization: 1,625\n",
      "  I-Disease: 1,504\n",
      "  I-Location: 752\n",
      "  I-Date: 637\n",
      "  I-HumanCount: 327\n",
      "  B-Disease: 286\n",
      "  B-Organization: 268\n",
      "  B-Location: 260\n",
      "  I-Pathogen: 255\n",
      "  I-Medication: 155\n",
      "  B-Date: 153\n",
      "  I-Symptom: 111\n",
      "  B-HumanCount: 86\n",
      "  B-Pathogen: 46\n",
      "  B-Symptom: 32\n",
      "  B-Medication: 20\n",
      "\n",
      "ğŸ’¾ Comprehensive metrics saved to /home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/xlmr_ner_model/evaluation_metrics.json\n",
      "ğŸ‰ Manual training and evaluation completed!\n",
      "âœ… No Accelerate library required!\n"
     ]
    }
   ],
   "source": [
    "def train_model_trainer(model, train_dataset, val_dataset, output_dir, tokenizer,\n",
    "                       num_epochs=6, batch_size=8, learning_rate=2e-5, \n",
    "                       seed=42, patience=2):\n",
    "    \"\"\"\n",
    "    Train a Khmer NER model using Hugging Face Trainer with early stopping and dynamic padding.\n",
    "    \n",
    "    Args:\n",
    "        model: The NER model to train (e.g., BERT-based model for Khmer NER)\n",
    "        train_dataset: Training dataset with input_ids, attention_mask, and labels\n",
    "        val_dataset: Validation dataset with input_ids, attention_mask, and labels\n",
    "        output_dir: Directory to save model checkpoints and logs\n",
    "        tokenizer: Tokenizer used for the model (required for data collator)\n",
    "        num_epochs: Maximum number of epochs to train\n",
    "        batch_size: Per-device batch size\n",
    "        learning_rate: Initial learning rate\n",
    "        seed: Random seed for reproducibility\n",
    "        patience: Number of epochs to wait for improvement before stopping\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    # Check device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"ğŸ–¥ï¸  Using device: {device}\")\n",
    "    if device.type == \"cuda\":\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Define data collator for dynamic padding\n",
    "    data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        label_pad_token_id=-100  # Standard for ignoring padding tokens in NER loss\n",
    "    )\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=min(500, len(train_dataset) // batch_size // 10),\n",
    "        eval_strategy=\"epoch\",  \n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "        seed=seed,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_strategy=\"epoch\",\n",
    "        report_to=\"none\",  # Disable TensorBoard to avoid dependency error\n",
    "        fp16=torch.cuda.is_available(),  # Enable mixed precision on GPU\n",
    "        gradient_accumulation_steps=2,  # Add gradient accumulation for stability\n",
    "        max_grad_norm=1.0,  # Gradient clipping to prevent exploding gradients\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,  # Add collator for dynamic padding\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=patience, early_stopping_threshold=0.001)],\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    print(f\"ğŸš€ Starting training for Khmer NER...\")\n",
    "    print(f\"   Epochs: {num_epochs}\")\n",
    "    print(f\"   Per-device batch size: {batch_size}\")\n",
    "    print(f\"   Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"   Effective batch size: {batch_size * training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"   Learning rate: {learning_rate}\")\n",
    "    print(f\"   Early stopping patience: {patience} epochs\")\n",
    "    \n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"âœ… Training completed!\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âŒ Training failed: {e}\")\n",
    "        if \"out of memory\" in str(e).lower() and device.type == \"cuda\":\n",
    "            print(\"ğŸ”„ GPU memory error. Try reducing batch_size or increasing gradient_accumulation_steps.\")\n",
    "            print(f\"   Current batch_size: {batch_size}\")\n",
    "            print(f\"   Current gradient_accumulation_steps: {training_args.gradient_accumulation_steps}\")\n",
    "            print(\"   Suggested: batch_size=2, gradient_accumulation_steps=4\")\n",
    "        raise e\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "    model.save_pretrained(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    print(f\"ğŸ’¾ Final model saved to {final_model_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model_standard(model, tokenizer, test_dataset, id2label, output_dir, batch_size=8):\n",
    "    \"\"\"\n",
    "    Standard evaluation for NER models following best practices.\n",
    "    Uses both token-level and entity-level (seqeval) metrics.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” Evaluating model on test set...\")\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    # Create data loader with proper collate function\n",
    "    def collate_fn_eval(batch):\n",
    "        \"\"\"Custom collate function for evaluation DataLoader with padding\"\"\"\n",
    "        max_length = max(len(item['input_ids']) for item in batch)\n",
    "        \n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        for item in batch:\n",
    "            seq_len = len(item['input_ids'])\n",
    "            padding_length = max_length - seq_len\n",
    "            \n",
    "            # Use tokenizer's pad_token_id, fallback to 0\n",
    "            pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "            \n",
    "            # Pad sequences\n",
    "            padded_input_ids = torch.cat([\n",
    "                item['input_ids'], \n",
    "                torch.full((padding_length,), pad_token_id, dtype=item['input_ids'].dtype)\n",
    "            ])\n",
    "            \n",
    "            padded_attention_mask = torch.cat([\n",
    "                item['attention_mask'],\n",
    "                torch.zeros(padding_length, dtype=item['attention_mask'].dtype)\n",
    "            ])\n",
    "            \n",
    "            # Pad labels with -100 (ignore index for loss calculation)\n",
    "            padded_labels = torch.cat([\n",
    "                item['labels'],\n",
    "                torch.full((padding_length,), -100, dtype=item['labels'].dtype)\n",
    "            ])\n",
    "            \n",
    "            input_ids_list.append(padded_input_ids)\n",
    "            attention_mask_list.append(padded_attention_mask)\n",
    "            labels_list.append(padded_labels)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.stack(input_ids_list),\n",
    "            'attention_mask': torch.stack(attention_mask_list),\n",
    "            'labels': torch.stack(labels_list)\n",
    "        }\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn_eval,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Collections for different evaluation approaches\n",
    "    all_predictions = []\n",
    "    seqeval_true_labels = []  # For entity-level evaluation\n",
    "    seqeval_pred_labels = []  # For entity-level evaluation\n",
    "    token_true_labels = []    # For token-level evaluation  \n",
    "    token_pred_labels = []    # For token-level evaluation\n",
    "    \n",
    "    print(\"Running inference...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            # Get model predictions\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "            true_labels = labels.cpu().numpy()\n",
    "            \n",
    "            # Process each sample in the batch\n",
    "            for sample_idx in range(input_ids.size(0)):\n",
    "                tokens = tokenizer.convert_ids_to_tokens(input_ids[sample_idx].cpu())\n",
    "                sample_predictions = predictions[sample_idx]\n",
    "                sample_labels = true_labels[sample_idx]\n",
    "                \n",
    "                # Extract valid predictions (ignore padding and special tokens)\n",
    "                seq_pred_tags = []\n",
    "                seq_true_tags = []\n",
    "                valid_tokens = []\n",
    "                \n",
    "                for pred_id, true_id, token in zip(sample_predictions, sample_labels, tokens):\n",
    "                    # Skip padded tokens and special tokens\n",
    "                    if true_id != -100 and not token.startswith('[') and token not in ['<s>', '</s>', '<pad>', '<unk>']:\n",
    "                        pred_tag = id2label[pred_id]\n",
    "                        true_tag = id2label[true_id]\n",
    "                        \n",
    "                        seq_pred_tags.append(pred_tag)\n",
    "                        seq_true_tags.append(true_tag)\n",
    "                        valid_tokens.append(token)\n",
    "                        \n",
    "                        # For token-level metrics\n",
    "                        token_pred_labels.append(pred_tag)\n",
    "                        token_true_labels.append(true_tag)\n",
    "                \n",
    "                # Add to seqeval collections (sequence-level)\n",
    "                if seq_pred_tags and seq_true_tags:\n",
    "                    seqeval_pred_labels.append(seq_pred_tags)\n",
    "                    seqeval_true_labels.append(seq_true_tags)\n",
    "                    \n",
    "                    # Store detailed predictions\n",
    "                    all_predictions.append({\n",
    "                        'tokens': valid_tokens,\n",
    "                        'true_tags': seq_true_tags,\n",
    "                        'pred_tags': seq_pred_tags\n",
    "                    })\n",
    "    \n",
    "    print(f\"âœ… Evaluation completed on {len(all_predictions)} samples\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save detailed predictions\n",
    "    save_predictions(all_predictions, output_dir)\n",
    "    \n",
    "    # Calculate and display metrics\n",
    "    metrics = calculate_comprehensive_metrics(\n",
    "        seqeval_true_labels, seqeval_pred_labels,\n",
    "        token_true_labels, token_pred_labels,\n",
    "        output_dir\n",
    "    )\n",
    "    \n",
    "    return all_predictions, seqeval_true_labels, seqeval_pred_labels, metrics\n",
    "\n",
    "def save_predictions(all_predictions, output_dir):\n",
    "    \"\"\"Save predictions in multiple formats\"\"\"\n",
    "    \n",
    "    # Save as JSON\n",
    "    json_path = os.path.join(output_dir, \"predictions.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_predictions, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"ğŸ’¾ Detailed predictions saved to {json_path}\")\n",
    "    \n",
    "    # Save as CoNLL format\n",
    "    conll_path = os.path.join(output_dir, \"predictions.conll\")\n",
    "    with open(conll_path, 'w', encoding='utf-8') as f:\n",
    "        for pred in all_predictions:\n",
    "            for token, true_tag, pred_tag in zip(pred['tokens'], pred['true_tags'], pred['pred_tags']):\n",
    "                f.write(f\"{token}\\t{true_tag}\\t{pred_tag}\\n\")\n",
    "            f.write(\"\\n\")  # Empty line between sentences\n",
    "    print(f\"ğŸ’¾ CoNLL format saved to {conll_path}\")\n",
    "\n",
    "def serialize_classification_report(report_dict):\n",
    "    \"\"\"Convert classification report to JSON-serializable format\"\"\"\n",
    "    serializable = {}\n",
    "    for key, value in report_dict.items():\n",
    "        if isinstance(key, tuple):\n",
    "            # Convert tuple keys to string representation\n",
    "            key = str(key)\n",
    "        elif key is None:\n",
    "            key = \"none\"\n",
    "        \n",
    "        if isinstance(value, dict):\n",
    "            serializable[key] = serialize_classification_report(value)\n",
    "        else:\n",
    "            serializable[key] = value\n",
    "    return serializable\n",
    "\n",
    "def analyze_confusion_patterns(true_labels, pred_labels):\n",
    "    \"\"\"Analyze confusion patterns between predictions and true labels\"\"\"\n",
    "    confusion_patterns = {}\n",
    "    \n",
    "    # Count misclassifications\n",
    "    misclassifications = Counter()\n",
    "    correct_predictions = Counter()\n",
    "    \n",
    "    for true_label, pred_label in zip(true_labels, pred_labels):\n",
    "        if true_label == pred_label:\n",
    "            correct_predictions[true_label] += 1\n",
    "        else:\n",
    "            misclassifications[(true_label, pred_label)] += 1\n",
    "    \n",
    "    # Convert to serializable format\n",
    "    confusion_patterns['misclassifications'] = {f\"{true_label}â†’{pred_label}\": count \n",
    "                                                for (true_label, pred_label), count in misclassifications.most_common(10)}\n",
    "    confusion_patterns['correct_predictions'] = dict(correct_predictions)\n",
    "    \n",
    "    return confusion_patterns\n",
    "\n",
    "def calculate_comprehensive_metrics(seqeval_true_labels, seqeval_pred_labels, \n",
    "                                  token_true_labels, token_pred_labels, output_dir):\n",
    "    \"\"\"Calculate comprehensive NER evaluation metrics\"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ“Š Comprehensive NER Evaluation Results:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Entity-level metrics (Standard for NER - using seqeval)\n",
    "    print(\"ğŸ¯ ENTITY-LEVEL METRICS (Primary - Standard for NER)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        entity_precision = seq_precision(seqeval_true_labels, seqeval_pred_labels)\n",
    "        entity_recall = seq_recall(seqeval_true_labels, seqeval_pred_labels) \n",
    "        entity_f1 = seq_f1(seqeval_true_labels, seqeval_pred_labels)\n",
    "        entity_accuracy = accuracy_score(seqeval_true_labels, seqeval_pred_labels)\n",
    "        \n",
    "        print(f\"Entity Precision: {entity_precision:.4f}\")\n",
    "        print(f\"Entity Recall:    {entity_recall:.4f}\")\n",
    "        print(f\"Entity F1-Score:  {entity_f1:.4f}\")\n",
    "        print(f\"Entity Accuracy:  {entity_accuracy:.4f}\")\n",
    "        \n",
    "        metrics.update({\n",
    "            'entity_precision': entity_precision,\n",
    "            'entity_recall': entity_recall,\n",
    "            'entity_f1_score': entity_f1,\n",
    "            'entity_accuracy': entity_accuracy\n",
    "        })\n",
    "        \n",
    "        # Detailed entity-level classification report\n",
    "        # Note: seqeval classification_report returns a string, not a dict\n",
    "        entity_report_str = seq_classification_report(seqeval_true_labels, seqeval_pred_labels)\n",
    "        print(f\"\\nğŸ“‹ Entity-level Classification Report:\")\n",
    "        print(entity_report_str)\n",
    "        \n",
    "        # Store the string report (seqeval doesn't provide dict output)\n",
    "        metrics['entity_classification_report'] = entity_report_str\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error calculating entity-level metrics: {e}\")\n",
    "        metrics['entity_error'] = str(e)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # 2. Token-level metrics\n",
    "    print(\"ğŸ”¤ TOKEN-LEVEL METRICS (Secondary)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Overall token accuracy\n",
    "        token_accuracy = sum(1 for t, p in zip(token_true_labels, token_pred_labels) if t == p) / len(token_true_labels)\n",
    "        print(f\"Token Accuracy: {token_accuracy:.4f}\")\n",
    "        \n",
    "        # Get unique labels for token-level metrics\n",
    "        unique_labels = sorted(list(set(token_true_labels + token_pred_labels)))\n",
    "        \n",
    "        # Token-level metrics (micro and macro averages)\n",
    "        token_precision_micro = precision_score(token_true_labels, token_pred_labels, \n",
    "                                               labels=unique_labels, average='micro', zero_division=0)\n",
    "        token_recall_micro = recall_score(token_true_labels, token_pred_labels, \n",
    "                                         labels=unique_labels, average='micro', zero_division=0)\n",
    "        token_f1_micro = f1_score(token_true_labels, token_pred_labels, \n",
    "                                 labels=unique_labels, average='micro', zero_division=0)\n",
    "        \n",
    "        token_precision_macro = precision_score(token_true_labels, token_pred_labels, \n",
    "                                               labels=unique_labels, average='macro', zero_division=0)\n",
    "        token_recall_macro = recall_score(token_true_labels, token_pred_labels, \n",
    "                                         labels=unique_labels, average='macro', zero_division=0)\n",
    "        token_f1_macro = f1_score(token_true_labels, token_pred_labels, \n",
    "                                 labels=unique_labels, average='macro', zero_division=0)\n",
    "        \n",
    "        print(f\"Token Precision (Micro): {token_precision_micro:.4f}\")\n",
    "        print(f\"Token Recall (Micro):    {token_recall_micro:.4f}\")\n",
    "        print(f\"Token F1-Score (Micro):  {token_f1_micro:.4f}\")\n",
    "        print(f\"Token Precision (Macro): {token_precision_macro:.4f}\")\n",
    "        print(f\"Token Recall (Macro):    {token_recall_macro:.4f}\")\n",
    "        print(f\"Token F1-Score (Macro):  {token_f1_macro:.4f}\")\n",
    "        \n",
    "        metrics.update({\n",
    "            'token_accuracy': token_accuracy,\n",
    "            'token_precision_micro': token_precision_micro,\n",
    "            'token_recall_micro': token_recall_micro,\n",
    "            'token_f1_micro': token_f1_micro,\n",
    "            'token_precision_macro': token_precision_macro,\n",
    "            'token_recall_macro': token_recall_macro,\n",
    "            'token_f1_macro': token_f1_macro\n",
    "        })\n",
    "        \n",
    "        # Token-level classification report (using sklearn)\n",
    "        token_report = classification_report(token_true_labels, token_pred_labels, \n",
    "                                           labels=unique_labels, zero_division=0, output_dict=True)\n",
    "        print(f\"\\nğŸ“‹ Token-level Classification Report:\")\n",
    "        print(classification_report(token_true_labels, token_pred_labels, \n",
    "                                   labels=unique_labels, zero_division=0))\n",
    "        \n",
    "        # Serialize the classification report for JSON storage\n",
    "        metrics['token_classification_report'] = serialize_classification_report(token_report)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error calculating token-level metrics: {e}\")\n",
    "        metrics['token_error'] = str(e)\n",
    "    \n",
    "    # 3. Additional analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“ˆ ADDITIONAL ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Count statistics\n",
    "    total_tokens = len(token_true_labels)\n",
    "    entity_tokens = sum(1 for label in token_true_labels if label != 'O')\n",
    "    o_tokens = total_tokens - entity_tokens\n",
    "    \n",
    "    print(f\"Total tokens: {total_tokens:,}\")\n",
    "    print(f\"Entity tokens: {entity_tokens:,} ({entity_tokens/total_tokens*100:.1f}%)\")\n",
    "    print(f\"O tokens: {o_tokens:,} ({o_tokens/total_tokens*100:.1f}%)\")\n",
    "    print(f\"Total sequences: {len(seqeval_true_labels):,}\")\n",
    "    \n",
    "    # Entity type distribution\n",
    "    entity_counts = Counter([label for label in token_true_labels if label != 'O'])\n",
    "    print(f\"\\nğŸ·ï¸ Entity Type Distribution:\")\n",
    "    for entity_type, count in entity_counts.most_common():\n",
    "        print(f\"  {entity_type}: {count:,}\")\n",
    "    \n",
    "    metrics.update({\n",
    "        'total_tokens': total_tokens,\n",
    "        'entity_tokens': entity_tokens,\n",
    "        'o_tokens': o_tokens,\n",
    "        'total_sequences': len(seqeval_true_labels),\n",
    "        'entity_distribution': dict(entity_counts)\n",
    "    })\n",
    "    \n",
    "    # 4. Error analysis\n",
    "    confusion_analysis = analyze_confusion_patterns(token_true_labels, token_pred_labels)\n",
    "    metrics['confusion_analysis'] = confusion_analysis\n",
    "    \n",
    "    # Save comprehensive metrics\n",
    "    metrics_path = os.path.join(output_dir, \"evaluation_metrics.json\")\n",
    "    with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics, f, ensure_ascii=False, indent=2, default=str)\n",
    "    print(f\"\\nğŸ’¾ Comprehensive metrics saved to {metrics_path}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "if 'model' in locals() and 'train_dataset' in locals():\n",
    "    print(\"ğŸ“ Starting manual model training...\")\n",
    "    trained_model = train_model_trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        output_dir=output_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        num_epochs=6,\n",
    "        batch_size=8,\n",
    "        learning_rate=2e-5\n",
    "    )\n",
    "    \n",
    "    # Then evaluate\n",
    "    print(\"ğŸ“Š Starting manual model evaluation...\")\n",
    "    predictions, true_tags, pred_tags, metrics = evaluate_model_standard(\n",
    "        model=trained_model,\n",
    "        tokenizer=tokenizer,\n",
    "        test_dataset=test_dataset,\n",
    "        id2label=id2label,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Model or datasets not ready. Please run the previous cells first.\")\n",
    "\n",
    "print(\"ğŸ‰ Manual training and evaluation completed!\")\n",
    "print(\"âœ… No Accelerate library required!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kheed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
