{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88ec6dfd",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34a3786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/Public/kheed/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForTokenClassification, BertTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorForTokenClassification\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    XLMRobertaForTokenClassification, \n",
    "    XLMRobertaTokenizerFast, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "\n",
    "from seqeval.metrics import (\n",
    "    precision_score as seq_precision,\n",
    "    recall_score as seq_recall,\n",
    "    f1_score as seq_f1,\n",
    "    accuracy_score,\n",
    "    classification_report as seq_classification_report\n",
    ")\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Some weights of.*were not initialized from the model checkpoint.*\")\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c6729c",
   "metadata": {},
   "source": [
    "# Define Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "357e75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "input_dir = \"/home/guest/Public/KHEED/KHEED_Data_Collection/Final/bio_tagged\"\n",
    "output_dir = \"/home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/Models/bert_khmer_ner_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a714ca",
   "metadata": {},
   "source": [
    "# Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9add024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_files_robust(input_dir, verbose=True):\n",
    "    \"\"\"\n",
    "    Load JSON files and extract tokens and BIO tags with robust error handling.\n",
    "    \"\"\"\n",
    "    all_tokens = []\n",
    "    all_tags = []\n",
    "    input_files = glob.glob(os.path.join(input_dir, \"*.json\"))\n",
    "    \n",
    "    if not input_files:\n",
    "        print(f\"‚ùå No JSON files found in '{input_dir}'. Please check the directory.\")\n",
    "        return [], [], []\n",
    "    \n",
    "    print(f\"üìÇ Processing {len(input_files)} files...\")\n",
    "    \n",
    "    errors = []\n",
    "    skipped_sentences = 0\n",
    "    processed_sentences = 0\n",
    "    \n",
    "    for file_idx, input_file in enumerate(input_files):\n",
    "        if verbose and file_idx < 5:\n",
    "            print(f\"  üìÑ Processing: {os.path.basename(input_file)}\")\n",
    "        \n",
    "        try:\n",
    "            with open(input_file, 'r', encoding='utf-8') as f:\n",
    "                obj = json.load(f)\n",
    "            \n",
    "            processed_content = obj.get('processed_content', [])\n",
    "            \n",
    "            for sent_idx, sentence_data in enumerate(processed_content):\n",
    "                try:\n",
    "                    tokens = sentence_data.get('tokens', [])\n",
    "                    bio_tags = sentence_data.get('bio_tags', [])\n",
    "                    \n",
    "                    if not tokens or not bio_tags:\n",
    "                        skipped_sentences += 1\n",
    "                        continue\n",
    "                    \n",
    "                    flattened_tags = [tag[0] if isinstance(tag, list) and tag else tag for tag in bio_tags]\n",
    "                    if len(tokens) != len(flattened_tags):\n",
    "                        error_msg = f\"Length mismatch in {os.path.basename(input_file)}, sentence {sent_idx}: {len(tokens)} tokens vs {len(flattened_tags)} tags\"\n",
    "                        errors.append(error_msg)\n",
    "                        if verbose and len(errors) <= 3:\n",
    "                            print(f\"    ‚ö†Ô∏è  {error_msg}\")\n",
    "                        skipped_sentences += 1\n",
    "                        continue\n",
    "                    \n",
    "                    validated_tags = [tag if tag and isinstance(tag, str) else \"O\" for tag in flattened_tags]\n",
    "                    all_tokens.append(tokens)\n",
    "                    all_tags.append(validated_tags)\n",
    "                    processed_sentences += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error processing sentence {sent_idx} in {os.path.basename(input_file)}: {e}\"\n",
    "                    errors.append(error_msg)\n",
    "                    if verbose and len(errors) <= 3:\n",
    "                        print(f\"    ‚ùå {error_msg}\")\n",
    "                    skipped_sentences += 1\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error reading file {input_file}: {e}\"\n",
    "            errors.append(error_msg)\n",
    "            if verbose:\n",
    "                print(f\"  ‚ùå {error_msg}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nüìä Data loading summary:\")\n",
    "    print(f\"  ‚úÖ Processed sentences: {processed_sentences}\")\n",
    "    print(f\"  ‚ö†Ô∏è  Skipped sentences: {skipped_sentences}\")\n",
    "    print(f\"  ‚ùå Total errors: {len(errors)}\")\n",
    "    \n",
    "    return all_tokens, all_tags, input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381b7468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing data structure...\n",
      "üìÅ Found 525 JSON files\n",
      "üìã Analyzing first 5 files...\n",
      "\n",
      "File 1: object_d2279a49-8b25-4e4c-b936-62f88487a895.json\n",
      "  üìù Keys: ['id', 'annotations', 'file_upload', 'drafts', 'predictions', 'data', 'meta', 'created_at', 'updated_at', 'inner_id', 'total_annotations', 'cancelled_annotations', 'total_predictions', 'comment_count', 'unresolved_comment_count', 'last_comment_updated_at', 'project', 'updated_by', 'comment_authors', 'processed_content']\n",
      "  üìä Sentences in file: 9\n",
      "    Sentence 1:\n",
      "      üî§ Tokens (81): ['·ûß·ûî', '·ûì·û∂·ûô·ûÄ', '·ûö·ûä·üí·ûã·ûò·ûì·üí·ûè·üí·ûö·û∏', ' ', '·ûì·üÅ·ûè', ' ', '·ûü·û∂·ûú·ûø·ûì', ' ', '·ûè·üÜ·ûé·û∂·ûÑ', '·ûä·üè', '·ûÅ·üí·ûñ·ûÑ·üã·ûÅ·üí·ûñ·ûü·üã', '·ûö·ûî·ûü·üã', '·ûü·ûò·üí·ûè·üÅ·ûÖ', '·ûò·û†·û∂', '·ûî·ûú·ûö·ûí·û∑·ûî·ûè·û∏', ' ', '·û†·üä·ûª·ûì', ' ', '·ûò·üâ·û∂·ûé·üÇ·ûè', ' ']...\n",
      "      üè∑Ô∏è  Tags (81): [['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "    Sentence 2:\n",
      "      üî§ Tokens (83): ['·ûê·üí·ûõ·üÇ·ûÑ', '·ûÄ·üí·ûì·ûª·ûÑ', '·ûñ·û∑·ûí·û∏', '·ûî·üí·ûö·ûÄ·û∂·ûü', '·ûá·û∂', '·ûï·üí·ûõ·ûº·ûú·ûÄ·û∂·ûö·ûä·û∂·ûÄ·üã', '·û±·üí·ûô', '·û¢·ûì·ûª·ûú·ûè·üí·ûè', '·ûÄ·û∂·ûö', ' ', '·ûÄ·üÇ·ûë·ûò·üí·ûö·ûÑ·üã', '·ûî·üí·ûö·ûñ·üê·ûì·üí·ûí', '·ûõ·ûÄ·üã', '·ûü·üÜ·ûî·ûª·ûè·üí·ûö', ' ', '·ûì·û∑·ûÑ', '·ûÄ·û∂·ûö·ûè·üí·ûö·ûΩ·ûè·ûñ·û∑·ûì·û∑·ûè·üí·ûô', '·ûü·üÜ·ûî·ûª·ûè·üí·ûö', '·ûÖ·ûº·ûõ', '·ûë·ûü·üí·ûü·ûì·û∂']...\n",
      "      üè∑Ô∏è  Tags (83): [['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "\n",
      "File 2: object_e1a96cb5-7830-4846-a601-0a0357497b28.json\n",
      "  üìù Keys: ['id', 'annotations', 'file_upload', 'drafts', 'predictions', 'data', 'meta', 'created_at', 'updated_at', 'inner_id', 'total_annotations', 'cancelled_annotations', 'total_predictions', 'comment_count', 'unresolved_comment_count', 'last_comment_updated_at', 'project', 'updated_by', 'comment_authors', 'processed_content']\n",
      "  üìä Sentences in file: 32\n",
      "    Sentence 1:\n",
      "      üî§ Tokens (43): ['·ûÄ·üí·ûö·ûü·ûΩ·ûÑ', '·ûü·ûª·ûÅ·û∂·ûó·û∑·ûî·û∂·ûõ', '·ûì·û∑·ûÑ', '·ûä·üÉ·ûÇ·ûº', '·ûñ·û∂·ûÄ·üã·ûñ·üê·ûì·üí·ûí', '·ûä·ûº·ûÖ·ûá·û∂', '·û¢·ûÑ·üí·ûÇ·ûÄ·û∂·ûö', '·ûô·ûº·ûì·û∏', '·ûü·üÅ·û†·üí·ûú', ' ', '(Unicef)', ' ', '·ûî·û∂·ûì', '·ûÖ·û∂·ûî·üã·ûï·üí·ûè·ûæ·ûò', '·ûî·ûæ·ûÄ', '·ûô·ûª·ûë·üí·ûí·ûì·û∂·ûÄ·û∂·ûö', '·ûÖ·û∂·ûÄ·üã', '·ûú·üâ·û∂·ûÄ·üã·ûü·û∂·üÜ·ûÑ', '·ûá·üÜ·ûÑ·û∫', '·ûÄ·ûâ·üí·ûö·üí·ûá·ûπ·ûõ']...\n",
      "      üè∑Ô∏è  Tags (43): [['B-Organization'], ['I-Organization'], ['O'], ['O'], ['O'], ['O'], ['B-Organization'], ['I-Organization'], ['I-Organization'], ['O'], ['B-Organization'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['B-Medication'], ['I-Medication'], ['I-Medication']]...\n",
      "    Sentence 2:\n",
      "      üî§ Tokens (34): ['·ûô·ûª·ûë·üí·ûí·ûì·û∂·ûÄ·û∂·ûö', '·ûì·üÅ·üá', '·ûÇ·û∫·ûá·û∂', '·ûÄ·û∂·ûö·ûÜ·üí·ûõ·ûæ·ûô·ûè·ûî', '·ûë·û∂·ûì·üã', '·ûñ·üÅ·ûõ·ûú·üÅ·ûõ·û∂', '·ûÖ·üÜ·ûñ·üÑ·üá', '·ûÄ·û∂·ûö·ûÄ·ûæ·ûì·û°·ûæ·ûÑ', '·ûá·û∂', '·ûü·ûÄ·ûõ', '·ûì·üÉ', '·ûÄ·ûö·ûé·û∏', '·ûá·üÜ·ûÑ·û∫', '·ûÄ·ûâ·üí·ûö·üí·ûá·ûπ·ûõ', '·ûü·üí·û¢·ûº·ûÖ', ' ', '·ûì·û∑·ûÑ', '·ûÄ·û∂·ûö·ûï·üí·ûë·ûª·üá·û°·ûæ·ûÑ', '·ûì·üÉ', '·ûá·üÜ·ûÑ·û∫']...\n",
      "      üè∑Ô∏è  Tags (34): [['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['B-Disease'], ['I-Disease'], ['O'], ['O'], ['O'], ['O']]...\n",
      "\n",
      "File 3: object_792495e7-39e6-4902-90b7-5edecd04877b.json\n",
      "  üìù Keys: ['id', 'annotations', 'file_upload', 'drafts', 'predictions', 'data', 'meta', 'created_at', 'updated_at', 'inner_id', 'total_annotations', 'cancelled_annotations', 'total_predictions', 'comment_count', 'unresolved_comment_count', 'last_comment_updated_at', 'project', 'updated_by', 'comment_authors', 'processed_content']\n",
      "  üìä Sentences in file: 11\n",
      "    Sentence 1:\n",
      "      üî§ Tokens (60): ['·ûõ·üÑ·ûÄ', ' ', '·ûÇ·û∏·ûò', ' ', '·ûö·û∑·ûë·üí·ûí·û∏', ' ', '·û¢·ûó·û∑·ûî·û∂·ûõ', '·ûÅ·üÅ·ûè·üí·ûè', '·ûñ·üí·ûö·üá·ûú·û∑·û†·û∂·ûö', ' ', '·ûî·û∂·ûì', '·ûê·üí·ûõ·üÇ·ûÑ', '·ûê·û∂', ' ', '·ûÇ·û∑·ûè', '·ûò·ûÄ·ûä·ûõ·üã', '·ûê·üí·ûÑ·üÉ', '·ûì·üÅ·üá', '·ûõ·üÑ·ûÄ', '·ûî·û∂·ûì']...\n",
      "      üè∑Ô∏è  Tags (60): [['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['B-Location'], ['I-Location'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "    Sentence 2:\n",
      "      üî§ Tokens (88): ['·ûÄ·û∂·ûö·ûê·üí·ûõ·üÇ·ûÑ', '·ûö·ûî·ûü·üã', '·ûõ·üÑ·ûÄ', ' ', '·ûÇ·û∏·ûò', ' ', '·ûö·û∑·ûë·üí·ûí·û∏', ' ', '·ûÄ·üí·ûì·ûª·ûÑ', '·û±·ûÄ·û∂·ûü', '·ûä·üÇ·ûõ', '·ûõ·üÑ·ûÄ', '·û¢·ûâ·üí·ûá·ûæ·ûâ', '·ûÖ·ûº·ûõ·ûö·ûΩ·ûò', '·ûÄ·üí·ûì·ûª·ûÑ', '·ûñ·û∑·ûí·û∏', '·ûî·ûö·û∑·ûÖ·üí·ûÅ·û∂·ûÄ', '·ûà·û∂·ûò', '·ûä·üÑ·ûô', '·ûü·üí·ûò·üê·ûÇ·üí·ûö·ûÖ·û∑·ûè·üí·ûè']...\n",
      "      üè∑Ô∏è  Tags (88): [['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "\n",
      "File 4: object_1fe657cc-df5b-4309-aaf2-e25e485b8ff4.json\n",
      "  üìù Keys: ['id', 'annotations', 'file_upload', 'drafts', 'predictions', 'data', 'meta', 'created_at', 'updated_at', 'inner_id', 'total_annotations', 'cancelled_annotations', 'total_predictions', 'comment_count', 'unresolved_comment_count', 'last_comment_updated_at', 'project', 'updated_by', 'comment_authors', 'processed_content']\n",
      "  üìä Sentences in file: 10\n",
      "    Sentence 1:\n",
      "      üî§ Tokens (44): ['·ûõ·üÑ·ûÄ', '·ûü·û∂·ûü·üí·ûä·üí·ûö·û∂·ûÖ·û∂·ûö·üí·ûô', ' ', '·ûà·û∂·ûÑ', ' ', '·ûö·üâ·û∂', ' ', '·ûö·ûä·üí·ûã·ûò·ûì·üí·ûä·üí·ûö·û∏', '·ûÄ·üí·ûö·ûü·ûΩ·ûÑ', '·ûü·ûª·ûÅ·û∂·ûó·û∑·ûî·û∂·ûõ', ' ', '·ûî·û∂·ûì', '·ûê·üí·ûõ·üÇ·ûÑ', '·ûê·û∂', ' ', '·ûä·ûæ·ûò·üí·ûî·û∏', '·ûÜ·üí·ûõ·ûæ·ûô·ûè·ûî', '·ûë·üÖ·ûì·ûπ·ûÑ', '·ûè·ûò·üí·ûö·ûº·ûú·ûÄ·û∂·ûö', '·ûö·ûî·ûü·üã']...\n",
      "      üè∑Ô∏è  Tags (44): [['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['B-Organization'], ['I-Organization'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "    Sentence 2:\n",
      "      üî§ Tokens (45): ['·ûÄ·üí·ûì·ûª·ûÑ', '·ûì·üÑ·üá', '·ûÄ·û∂·ûö·ûñ·ûÑ·üí·ûö·ûπ·ûÑ', '·ûü·üÜ·ûä·üÖ', '·ûõ·ûæ', '·ûÄ·û∂·ûö·ûõ·ûæ·ûÄ·ûÄ·ûò·üí·ûñ·ûü·üã', '·ûÇ·ûª·ûé·ûó·û∂·ûñ', '·ûü·üÅ·ûú·û∂', ' ', '·û†·ûæ·ûô', '·ûÄ·û∂·ûö·ûñ·ûÑ·üí·ûö·û∏·ûÄ', ' ', '·ûÇ·û∫', '·ûÄ·û∂·ûö·ûü·ûò·üí·ûö·ûΩ·ûõ', '·û±·üí·ûô', '·ûò·û∂·ûì', '·ûÄ·û∂·ûö·ûî·ûò·üí·ûö·ûæ', '·ûü·üÅ·ûú·û∂', '·ûü·ûª·ûÅ·ûó·û∂·ûñ', '·û±·üí·ûô']...\n",
      "      üè∑Ô∏è  Tags (45): [['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "\n",
      "File 5: object_04d28d27-96e9-4d95-b946-c1475b2207d2.json\n",
      "  üìù Keys: ['id', 'annotations', 'file_upload', 'drafts', 'predictions', 'data', 'meta', 'created_at', 'updated_at', 'inner_id', 'total_annotations', 'cancelled_annotations', 'total_predictions', 'comment_count', 'unresolved_comment_count', 'last_comment_updated_at', 'project', 'updated_by', 'comment_authors', 'processed_content']\n",
      "  üìä Sentences in file: 5\n",
      "    Sentence 1:\n",
      "      üî§ Tokens (64): ['·ûÄ·û∂·ûö·ûõ·ûæ·ûÄ·û°·ûæ·ûÑ', '·ûö·ûî·ûü·üã', ' ', '·ûõ·üÑ·ûÄ', ' ', '·ûß·ûè·üí·ûè·ûò', '·û¢·ûÇ·üí·ûÇ·û∂·ûì·ûª·ûö·ûÄ·üí·ûü', '·ûê·üí·ûì·û∂·ûÄ·üã', '·ûõ·üÅ·ûÅ', '·ü°', ' ', '·ûà·ûì', ' ', '·ûü·û∂·ûé·û∂·ûè', ' ', '·ûì·û∂', '·û±·ûÄ·û∂·ûü', '·û¢·ûâ·üí·ûá·ûæ·ûâ', '·ûÖ·ûª·üá', '·ûè·üí·ûö·ûΩ·ûè·ûñ·û∑·ûì·û∑·ûè·üí·ûô']...\n",
      "      üè∑Ô∏è  Tags (64): [['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "    Sentence 2:\n",
      "      üî§ Tokens (47): ['·ûõ·üÑ·ûÄ', '·û¢·ûÇ·üí·ûÇ', '·ûì·û∂·ûô·ûÄ', ' ', '·ûî·û∂·ûì', '·ûî·û∂·ûì', '·ûí·üí·ûú·ûæ', '·ûÄ·û∂·ûö·ûü·üÜ·ûé·üÅ·üá·ûü·üÜ·ûé·û∂·ûõ', ' ', '·ûì·û∑·ûÑ', '·ûò·û∂·ûì·ûî·üí·ûö·ûü·û∂·ûü·ûì·üç', '·ûé·üÇ·ûì·û∂·üÜ', '·ûë·üÖ·ûÄ·û∂·ûì·üã', '·ûá·ûì', '·ûá·û∂·ûî·üã', '·ûÉ·ûª·üÜ', '·ûä·üÇ·ûõ', '·ûá·û∂', '·ûÄ·üí·ûò·üÅ·ûÑ', '·ûë·üÜ·ûì·ûæ·ûÑ']...\n",
      "      üè∑Ô∏è  Tags (47): [['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O']]...\n",
      "\n",
      "üè∑Ô∏è  Unique tags found: ['B-Date', 'B-Disease', 'B-HumanCount', 'B-Location', 'B-Medication', 'B-Organization', 'I-Date', 'I-Disease', 'I-HumanCount', 'I-Location', 'I-Medication', 'I-Organization', 'O']\n",
      "üìà Total unique tags: 13\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "def analyze_data_structure(input_dir, max_files_to_check=5):\n",
    "    \"\"\"\n",
    "    Analyze the structure of JSON files to understand the data format.\n",
    "    \"\"\"\n",
    "    input_files = glob.glob(os.path.join(input_dir, \"*.json\"))\n",
    "    \n",
    "    if not input_files:\n",
    "        print(f\"‚ùå No JSON files found in '{input_dir}'\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìÅ Found {len(input_files)} JSON files\")\n",
    "    print(f\"üìã Analyzing first {min(max_files_to_check, len(input_files))} files...\\n\")\n",
    "    \n",
    "    all_tags = set()\n",
    "    sample_data = []\n",
    "    \n",
    "    for i, file_path in enumerate(input_files[:max_files_to_check]):\n",
    "        print(f\"File {i+1}: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Show file structure\n",
    "            print(f\"  üìù Keys: {list(data.keys())}\")\n",
    "            \n",
    "            if 'processed_content' in data:\n",
    "                processed_content = data['processed_content']\n",
    "                print(f\"  üìä Sentences in file: {len(processed_content)}\")\n",
    "                \n",
    "                # Check first few sentences\n",
    "                for j, sentence in enumerate(processed_content[:2]):\n",
    "                    tokens = sentence.get('tokens', [])\n",
    "                    bio_tags = sentence.get('bio_tags', [])\n",
    "                    \n",
    "                    print(f\"    Sentence {j+1}:\")\n",
    "                    print(f\"      üî§ Tokens ({len(tokens)}): {tokens[:20]}{'...' if len(tokens) > 5 else ''}\")\n",
    "                    print(f\"      üè∑Ô∏è  Tags ({len(bio_tags)}): {bio_tags[:20]}{'...' if len(bio_tags) > 5 else ''}\")\n",
    "                    \n",
    "                    # Flatten tags and collect unique ones\n",
    "                    flat_tags = [tag[0] if isinstance(tag, list) and tag else tag for tag in bio_tags]\n",
    "                    all_tags.update(flat_tags)\n",
    "                    \n",
    "                    if j == 0:  # Save first sentence as sample\n",
    "                        sample_data.append({\n",
    "                            'file': os.path.basename(file_path),\n",
    "                            'tokens': tokens,\n",
    "                            'bio_tags': flat_tags\n",
    "                        })\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error reading file: {e}\\n\")\n",
    "    \n",
    "    print(f\"üè∑Ô∏è  Unique tags found: {sorted(all_tags)}\")\n",
    "    print(f\"üìà Total unique tags: {len(all_tags)}\")\n",
    "    \n",
    "    return all_tags, sample_data\n",
    "\n",
    "# %%\n",
    "\n",
    "print(\"üîç Analyzing data structure...\")\n",
    "unique_tags, sample_data = analyze_data_structure(input_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660f8aa",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fcb9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_robust(tokens, tags, tokenizer, label2id, max_length=512):\n",
    "    \"\"\"\n",
    "    Prepare dataset for BERT with robust error handling.\n",
    "    \"\"\"\n",
    "    encodings = []\n",
    "    skipped = 0\n",
    "    \n",
    "    print(f\"üîÑ Preparing dataset...\")\n",
    "    \n",
    "    for i, (token_list, tag_list) in enumerate(zip(tokens, tags)):\n",
    "        try:\n",
    "            if not token_list or not tag_list or len(token_list) != len(tag_list):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            missing_tags = [tag for tag in tag_list if tag not in label2id]\n",
    "            if missing_tags:\n",
    "                print(f\"‚ö†Ô∏è  Sentence {i}: Missing tags {set(missing_tags)} - replacing with 'O'\")\n",
    "                tag_list = [tag if tag in label2id else 'O' for tag in tag_list]\n",
    "            \n",
    "            encoding = tokenizer(\n",
    "                token_list,\n",
    "                is_split_into_words=True,\n",
    "                return_offsets_mapping=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=False\n",
    "            )\n",
    "            \n",
    "            aligned_labels = []\n",
    "            word_ids = encoding.word_ids()\n",
    "            prev_word_id = None\n",
    "            \n",
    "            for word_id in word_ids:\n",
    "                if word_id is None:\n",
    "                    aligned_labels.append(-100)\n",
    "                elif word_id != prev_word_id:\n",
    "                    if word_id < len(tag_list):\n",
    "                        aligned_labels.append(label2id[tag_list[word_id]])\n",
    "                    else:\n",
    "                        aligned_labels.append(label2id['O'])\n",
    "                else:\n",
    "                    if word_id < len(tag_list):\n",
    "                        tag = tag_list[word_id]\n",
    "                        if tag.startswith(\"B-\"):\n",
    "                            i_tag = f\"I-{tag[2:]}\"\n",
    "                            aligned_labels.append(label2id.get(i_tag, label2id[tag]))\n",
    "                        else:\n",
    "                            aligned_labels.append(label2id[tag])\n",
    "                    else:\n",
    "                        aligned_labels.append(label2id['O'])\n",
    "                \n",
    "                prev_word_id = word_id\n",
    "            \n",
    "            encoding[\"labels\"] = aligned_labels\n",
    "            del encoding[\"offset_mapping\"]\n",
    "            encodings.append(encoding)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing sentence {i}: {e}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"‚úÖ Prepared {len(encodings)} examples, skipped {skipped}\")\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a75489",
   "metadata": {},
   "source": [
    "# Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88f5656",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom Dataset for NER.\"\"\"\n",
    "    \n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec39fd",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61a01b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_trainer(model, train_dataset, val_dataset, output_dir, tokenizer,\n",
    "                       num_epochs=10, batch_size=4, learning_rate=2e-5, \n",
    "                       seed=42, patience=3):\n",
    "    \"\"\"\n",
    "    Train a Khmer NER model using Hugging Face Trainer with early stopping and dynamic padding.\n",
    "    \n",
    "    Args:\n",
    "        model: The NER model to train (e.g., BERT-based model for Khmer NER)\n",
    "        train_dataset: Training dataset with input_ids, attention_mask, and labels\n",
    "        val_dataset: Validation dataset with input_ids, attention_mask, and labels\n",
    "        output_dir: Directory to save model checkpoints and logs\n",
    "        tokenizer: Tokenizer used for the model (required for data collator)\n",
    "        num_epochs: Maximum number of epochs to train\n",
    "        batch_size: Per-device batch size\n",
    "        learning_rate: Initial learning rate\n",
    "        seed: Random seed for reproducibility\n",
    "        patience: Number of epochs to wait for improvement before stopping\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    # Check device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "    if device.type == \"cuda\":\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Define data collator for dynamic padding\n",
    "    data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        label_pad_token_id=-100  # Standard for ignoring padding tokens in NER loss\n",
    "    )\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=min(500, len(train_dataset) // batch_size // 10),\n",
    "        eval_strategy=\"epoch\",  \n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "        seed=seed,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_strategy=\"epoch\",\n",
    "        report_to=\"none\",  # Disable TensorBoard to avoid dependency error\n",
    "        fp16=torch.cuda.is_available(),  # Enable mixed precision on GPU\n",
    "        gradient_accumulation_steps=2,  # Add gradient accumulation for stability\n",
    "        max_grad_norm=1.0,  # Gradient clipping to prevent exploding gradients\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,  # Add collator for dynamic padding\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=patience, early_stopping_threshold=0.001)],\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    print(f\"üöÄ Starting training for Khmer NER...\")\n",
    "    print(f\"   Epochs: {num_epochs}\")\n",
    "    print(f\"   Per-device batch size: {batch_size}\")\n",
    "    print(f\"   Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"   Effective batch size: {batch_size * training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"   Learning rate: {learning_rate}\")\n",
    "    print(f\"   Early stopping patience: {patience} epochs\")\n",
    "    \n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"‚úÖ Training completed!\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\n",
    "        if \"out of memory\" in str(e).lower() and device.type == \"cuda\":\n",
    "            print(\"üîÑ GPU memory error. Try reducing batch_size or increasing gradient_accumulation_steps.\")\n",
    "            print(f\"   Current batch_size: {batch_size}\")\n",
    "            print(f\"   Current gradient_accumulation_steps: {training_args.gradient_accumulation_steps}\")\n",
    "            print(\"   Suggested: batch_size=2, gradient_accumulation_steps=4\")\n",
    "        raise e\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "    model.save_pretrained(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    print(f\"üíæ Final model saved to {final_model_path}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe96cb",
   "metadata": {},
   "source": [
    "# Evaluate Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85dc4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "def evaluate_model_standard(model, tokenizer, test_dataset, id2label, output_dir, batch_size=8):\n",
    "    \"\"\"\n",
    "    Standard evaluation for NER models following best practices.\n",
    "    Uses both token-level and entity-level (seqeval) metrics.\n",
    "    \"\"\"\n",
    "    print(\"üîç Evaluating model on test set...\")\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    # Create data loader with proper collate function\n",
    "    def collate_fn_eval(batch):\n",
    "        \"\"\"Custom collate function for evaluation DataLoader with padding\"\"\"\n",
    "        max_length = max(len(item['input_ids']) for item in batch)\n",
    "        \n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        for item in batch:\n",
    "            seq_len = len(item['input_ids'])\n",
    "            padding_length = max_length - seq_len\n",
    "            \n",
    "            # Use tokenizer's pad_token_id, fallback to 0\n",
    "            pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "            \n",
    "            # Pad sequences\n",
    "            padded_input_ids = torch.cat([\n",
    "                item['input_ids'], \n",
    "                torch.full((padding_length,), pad_token_id, dtype=item['input_ids'].dtype)\n",
    "            ])\n",
    "            \n",
    "            padded_attention_mask = torch.cat([\n",
    "                item['attention_mask'],\n",
    "                torch.zeros(padding_length, dtype=item['attention_mask'].dtype)\n",
    "            ])\n",
    "            \n",
    "            # Pad labels with -100 (ignore index for loss calculation)\n",
    "            padded_labels = torch.cat([\n",
    "                item['labels'],\n",
    "                torch.full((padding_length,), -100, dtype=item['labels'].dtype)\n",
    "            ])\n",
    "            \n",
    "            input_ids_list.append(padded_input_ids)\n",
    "            attention_mask_list.append(padded_attention_mask)\n",
    "            labels_list.append(padded_labels)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.stack(input_ids_list),\n",
    "            'attention_mask': torch.stack(attention_mask_list),\n",
    "            'labels': torch.stack(labels_list)\n",
    "        }\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn_eval,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Collections for different evaluation approaches\n",
    "    all_predictions = []\n",
    "    seqeval_true_labels = []  # For entity-level evaluation\n",
    "    seqeval_pred_labels = []  # For entity-level evaluation\n",
    "    token_true_labels = []    # For token-level evaluation  \n",
    "    token_pred_labels = []    # For token-level evaluation\n",
    "    \n",
    "    print(\"Running inference...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            # Get model predictions\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "            true_labels = labels.cpu().numpy()\n",
    "            \n",
    "            # Process each sample in the batch\n",
    "            for sample_idx in range(input_ids.size(0)):\n",
    "                tokens = tokenizer.convert_ids_to_tokens(input_ids[sample_idx].cpu())\n",
    "                sample_predictions = predictions[sample_idx]\n",
    "                sample_labels = true_labels[sample_idx]\n",
    "                \n",
    "                # Extract valid predictions (ignore padding and special tokens)\n",
    "                seq_pred_tags = []\n",
    "                seq_true_tags = []\n",
    "                valid_tokens = []\n",
    "                \n",
    "                for pred_id, true_id, token in zip(sample_predictions, sample_labels, tokens):\n",
    "                    # Skip padded tokens and special tokens\n",
    "                    if true_id != -100 and not token.startswith('[') and token not in ['<s>', '</s>', '<pad>', '<unk>']:\n",
    "                        pred_tag = id2label[pred_id]\n",
    "                        true_tag = id2label[true_id]\n",
    "                        \n",
    "                        seq_pred_tags.append(pred_tag)\n",
    "                        seq_true_tags.append(true_tag)\n",
    "                        valid_tokens.append(token)\n",
    "                        \n",
    "                        # For token-level metrics\n",
    "                        token_pred_labels.append(pred_tag)\n",
    "                        token_true_labels.append(true_tag)\n",
    "                \n",
    "                # Add to seqeval collections (sequence-level)\n",
    "                if seq_pred_tags and seq_true_tags:\n",
    "                    seqeval_pred_labels.append(seq_pred_tags)\n",
    "                    seqeval_true_labels.append(seq_true_tags)\n",
    "                    \n",
    "                    # Store detailed predictions\n",
    "                    all_predictions.append({\n",
    "                        'tokens': valid_tokens,\n",
    "                        'true_tags': seq_true_tags,\n",
    "                        'pred_tags': seq_pred_tags\n",
    "                    })\n",
    "    \n",
    "    print(f\"‚úÖ Evaluation completed on {len(all_predictions)} samples\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save detailed predictions\n",
    "    save_predictions(all_predictions, output_dir)\n",
    "    \n",
    "    # Calculate and display metrics\n",
    "    metrics = calculate_comprehensive_metrics(\n",
    "        seqeval_true_labels, seqeval_pred_labels,\n",
    "        token_true_labels, token_pred_labels,\n",
    "        output_dir\n",
    "    )\n",
    "    \n",
    "    return all_predictions, seqeval_true_labels, seqeval_pred_labels, metrics\n",
    "\n",
    "def save_predictions(all_predictions, output_dir):\n",
    "    \"\"\"Save predictions in multiple formats\"\"\"\n",
    "    \n",
    "    # Save as JSON\n",
    "    json_path = os.path.join(output_dir, \"predictions.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_predictions, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"üíæ Detailed predictions saved to {json_path}\")\n",
    "    \n",
    "    # Save as CoNLL format\n",
    "    conll_path = os.path.join(output_dir, \"predictions.conll\")\n",
    "    with open(conll_path, 'w', encoding='utf-8') as f:\n",
    "        for pred in all_predictions:\n",
    "            for token, true_tag, pred_tag in zip(pred['tokens'], pred['true_tags'], pred['pred_tags']):\n",
    "                f.write(f\"{token}\\t{true_tag}\\t{pred_tag}\\n\")\n",
    "            f.write(\"\\n\")  # Empty line between sentences\n",
    "    print(f\"üíæ CoNLL format saved to {conll_path}\")\n",
    "\n",
    "def serialize_classification_report(report_dict):\n",
    "    \"\"\"Convert classification report to JSON-serializable format\"\"\"\n",
    "    serializable = {}\n",
    "    for key, value in report_dict.items():\n",
    "        if isinstance(key, tuple):\n",
    "            # Convert tuple keys to string representation\n",
    "            key = str(key)\n",
    "        elif key is None:\n",
    "            key = \"none\"\n",
    "        \n",
    "        if isinstance(value, dict):\n",
    "            serializable[key] = serialize_classification_report(value)\n",
    "        else:\n",
    "            serializable[key] = value\n",
    "    return serializable\n",
    "\n",
    "def analyze_confusion_patterns(true_labels, pred_labels):\n",
    "    \"\"\"Analyze confusion patterns between predictions and true labels\"\"\"\n",
    "    confusion_patterns = {}\n",
    "    \n",
    "    # Count misclassifications\n",
    "    misclassifications = Counter()\n",
    "    correct_predictions = Counter()\n",
    "    \n",
    "    for true_label, pred_label in zip(true_labels, pred_labels):\n",
    "        if true_label == pred_label:\n",
    "            correct_predictions[true_label] += 1\n",
    "        else:\n",
    "            misclassifications[(true_label, pred_label)] += 1\n",
    "    \n",
    "    # Convert to serializable format\n",
    "    confusion_patterns['misclassifications'] = {f\"{true_label}‚Üí{pred_label}\": count \n",
    "                                                for (true_label, pred_label), count in misclassifications.most_common(10)}\n",
    "    confusion_patterns['correct_predictions'] = dict(correct_predictions)\n",
    "    \n",
    "    return confusion_patterns\n",
    "\n",
    "def calculate_comprehensive_metrics(seqeval_true_labels, seqeval_pred_labels, \n",
    "                                  token_true_labels, token_pred_labels, output_dir):\n",
    "    \"\"\"Calculate comprehensive NER evaluation metrics\"\"\"\n",
    "    \n",
    "    print(\"\\nüìä Comprehensive NER Evaluation Results:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Entity-level metrics (Standard for NER - using seqeval)\n",
    "    print(\"üéØ ENTITY-LEVEL METRICS (Primary - Standard for NER)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        entity_precision = seq_precision(seqeval_true_labels, seqeval_pred_labels)\n",
    "        entity_recall = seq_recall(seqeval_true_labels, seqeval_pred_labels) \n",
    "        entity_f1 = seq_f1(seqeval_true_labels, seqeval_pred_labels)\n",
    "        entity_accuracy = accuracy_score(seqeval_true_labels, seqeval_pred_labels)\n",
    "        \n",
    "        print(f\"Entity Precision: {entity_precision:.4f}\")\n",
    "        print(f\"Entity Recall:    {entity_recall:.4f}\")\n",
    "        print(f\"Entity F1-Score:  {entity_f1:.4f}\")\n",
    "        print(f\"Entity Accuracy:  {entity_accuracy:.4f}\")\n",
    "        \n",
    "        metrics.update({\n",
    "            'entity_precision': entity_precision,\n",
    "            'entity_recall': entity_recall,\n",
    "            'entity_f1_score': entity_f1,\n",
    "            'entity_accuracy': entity_accuracy\n",
    "        })\n",
    "        \n",
    "        # Detailed entity-level classification report\n",
    "        # Note: seqeval classification_report returns a string, not a dict\n",
    "        entity_report_str = seq_classification_report(seqeval_true_labels, seqeval_pred_labels)\n",
    "        print(f\"\\nüìã Entity-level Classification Report:\")\n",
    "        print(entity_report_str)\n",
    "        \n",
    "        # Store the string report (seqeval doesn't provide dict output)\n",
    "        metrics['entity_classification_report'] = entity_report_str\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error calculating entity-level metrics: {e}\")\n",
    "        metrics['entity_error'] = str(e)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # 2. Token-level metrics\n",
    "    print(\"üî§ TOKEN-LEVEL METRICS (Secondary)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Overall token accuracy\n",
    "        token_accuracy = sum(1 for t, p in zip(token_true_labels, token_pred_labels) if t == p) / len(token_true_labels)\n",
    "        print(f\"Token Accuracy: {token_accuracy:.4f}\")\n",
    "        \n",
    "        # Get unique labels for token-level metrics\n",
    "        unique_labels = sorted(list(set(token_true_labels + token_pred_labels)))\n",
    "        \n",
    "        # Token-level metrics (micro and macro averages)\n",
    "        token_precision_micro = precision_score(token_true_labels, token_pred_labels, \n",
    "                                               labels=unique_labels, average='micro', zero_division=0)\n",
    "        token_recall_micro = recall_score(token_true_labels, token_pred_labels, \n",
    "                                         labels=unique_labels, average='micro', zero_division=0)\n",
    "        token_f1_micro = f1_score(token_true_labels, token_pred_labels, \n",
    "                                 labels=unique_labels, average='micro', zero_division=0)\n",
    "        \n",
    "        token_precision_macro = precision_score(token_true_labels, token_pred_labels, \n",
    "                                               labels=unique_labels, average='macro', zero_division=0)\n",
    "        token_recall_macro = recall_score(token_true_labels, token_pred_labels, \n",
    "                                         labels=unique_labels, average='macro', zero_division=0)\n",
    "        token_f1_macro = f1_score(token_true_labels, token_pred_labels, \n",
    "                                 labels=unique_labels, average='macro', zero_division=0)\n",
    "        \n",
    "        print(f\"Token Precision (Micro): {token_precision_micro:.4f}\")\n",
    "        print(f\"Token Recall (Micro):    {token_recall_micro:.4f}\")\n",
    "        print(f\"Token F1-Score (Micro):  {token_f1_micro:.4f}\")\n",
    "        print(f\"Token Precision (Macro): {token_precision_macro:.4f}\")\n",
    "        print(f\"Token Recall (Macro):    {token_recall_macro:.4f}\")\n",
    "        print(f\"Token F1-Score (Macro):  {token_f1_macro:.4f}\")\n",
    "        \n",
    "        metrics.update({\n",
    "            'token_accuracy': token_accuracy,\n",
    "            'token_precision_micro': token_precision_micro,\n",
    "            'token_recall_micro': token_recall_micro,\n",
    "            'token_f1_micro': token_f1_micro,\n",
    "            'token_precision_macro': token_precision_macro,\n",
    "            'token_recall_macro': token_recall_macro,\n",
    "            'token_f1_macro': token_f1_macro\n",
    "        })\n",
    "        \n",
    "        # Token-level classification report (using sklearn)\n",
    "        token_report = classification_report(token_true_labels, token_pred_labels, \n",
    "                                           labels=unique_labels, zero_division=0, output_dict=True)\n",
    "        print(f\"\\nüìã Token-level Classification Report:\")\n",
    "        print(classification_report(token_true_labels, token_pred_labels, \n",
    "                                   labels=unique_labels, zero_division=0))\n",
    "        \n",
    "        # Serialize the classification report for JSON storage\n",
    "        metrics['token_classification_report'] = serialize_classification_report(token_report)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error calculating token-level metrics: {e}\")\n",
    "        metrics['token_error'] = str(e)\n",
    "    \n",
    "    # 3. Additional analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìà ADDITIONAL ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Count statistics\n",
    "    total_tokens = len(token_true_labels)\n",
    "    entity_tokens = sum(1 for label in token_true_labels if label != 'O')\n",
    "    o_tokens = total_tokens - entity_tokens\n",
    "    \n",
    "    print(f\"Total tokens: {total_tokens:,}\")\n",
    "    print(f\"Entity tokens: {entity_tokens:,} ({entity_tokens/total_tokens*100:.1f}%)\")\n",
    "    print(f\"O tokens: {o_tokens:,} ({o_tokens/total_tokens*100:.1f}%)\")\n",
    "    print(f\"Total sequences: {len(seqeval_true_labels):,}\")\n",
    "    \n",
    "    # Entity type distribution\n",
    "    entity_counts = Counter([label for label in token_true_labels if label != 'O'])\n",
    "    print(f\"\\nüè∑Ô∏è Entity Type Distribution:\")\n",
    "    for entity_type, count in entity_counts.most_common():\n",
    "        print(f\"  {entity_type}: {count:,}\")\n",
    "    \n",
    "    metrics.update({\n",
    "        'total_tokens': total_tokens,\n",
    "        'entity_tokens': entity_tokens,\n",
    "        'o_tokens': o_tokens,\n",
    "        'total_sequences': len(seqeval_true_labels),\n",
    "        'entity_distribution': dict(entity_counts)\n",
    "    })\n",
    "    \n",
    "    # 4. Error analysis\n",
    "    confusion_analysis = analyze_confusion_patterns(token_true_labels, token_pred_labels)\n",
    "    metrics['confusion_analysis'] = confusion_analysis\n",
    "    \n",
    "    # Save comprehensive metrics\n",
    "    metrics_path = os.path.join(output_dir, \"evaluation_metrics.json\")\n",
    "    with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics, f, ensure_ascii=False, indent=2, default=str)\n",
    "    print(f\"\\nüíæ Comprehensive metrics saved to {metrics_path}\")\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de64179a",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40a6061c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading data...\n",
      "üìÇ Processing 525 files...\n",
      "  üìÑ Processing: object_d2279a49-8b25-4e4c-b936-62f88487a895.json\n",
      "  üìÑ Processing: object_e1a96cb5-7830-4846-a601-0a0357497b28.json\n",
      "  üìÑ Processing: object_792495e7-39e6-4902-90b7-5edecd04877b.json\n",
      "  üìÑ Processing: object_1fe657cc-df5b-4309-aaf2-e25e485b8ff4.json\n",
      "  üìÑ Processing: object_04d28d27-96e9-4d95-b946-c1475b2207d2.json\n",
      "\n",
      "üìä Data loading summary:\n",
      "  ‚úÖ Processed sentences: 6221\n",
      "  ‚ö†Ô∏è  Skipped sentences: 12\n",
      "  ‚ùå Total errors: 0\n",
      "‚úÖ Successfully loaded 6221 sentences from 525 files\n",
      "‚ö†Ô∏è Warning: 213 duplicate sentences found:\n",
      "  Sentence: '·ûõ·üÑ·ûÄ ·ûê·üí·ûõ·üÇ·ûÑ ·ûê·û∂·üñ...' (repeated 8 times)\n",
      "  Sentence: '·ûõ·üÑ·ûÄ ·ûê·üí·ûõ·üÇ·ûÑ ·ûä·üÑ·ûô ·ûî·üí·ûö·üÇ ·ûü·ûò·üí·ûö·ûΩ·ûõ ·ûá·û∂ ·ûó·û∂·ûü·û∂ ·ûÅ·üí·ûò·üÇ·ûö ·ûê·û∂·üñ...' (repeated 2 times)\n",
      "  Sentence: '·ü°·üñ...' (repeated 11 times)\n",
      "  Sentence: '( ·ü°) ·üñ...' (repeated 2 times)\n",
      "  Sentence: '·ûÄ·üí·ûö·ûü·ûΩ·ûÑ ·ûü·ûª·ûÅ·û∂·ûó·û∑·ûî·û∂·ûõ   ·ûî·û∂·ûì ·û¢·üá·û¢·û∂·ûÑ ·ûê·û∂   ·ûÄ·û∂·ûö·ûï·üí·ûü·ûñ·üí·ûú·ûï·üí·ûü·û∂·ûô ·ûï...' (repeated 2 times)\n",
      "üè∑Ô∏è Label mapping created:\n",
      "  Total labels: 17\n",
      "  Labels: ['B-Date', 'B-Disease', 'B-HumanCount', 'B-Location', 'B-Medication', 'B-Organization', 'B-Pathogen', 'B-Symptom', 'I-Date', 'I-Disease', 'I-HumanCount', 'I-Location', 'I-Medication', 'I-Organization', 'I-Pathogen', 'I-Symptom', 'O']\n",
      "\n",
      "üìä Data split:\n",
      "  Training: 4,976 sentences\n",
      "  Validation: 622 sentences\n",
      "  Test: 623 sentences\n",
      "\n",
      "üìä Training entity distribution:\n",
      "+----------------+---------+--------------+\n",
      "| Entity         |   Count | Percentage   |\n",
      "+================+=========+==============+\n",
      "| B-Date         |    1082 | 3.99%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Disease      |    2594 | 9.55%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-HumanCount   |     686 | 2.53%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Location     |    2285 | 8.42%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Medication   |     179 | 0.66%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Organization |    2494 | 9.19%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Pathogen     |     387 | 1.43%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Symptom      |     391 | 1.44%        |\n",
      "+----------------+---------+--------------+\n",
      "| I-Date         |    3348 | 12.33%       |\n",
      "+----------------+---------+--------------+\n",
      "| I-Disease      |    3713 | 13.68%       |\n",
      "+----------------+---------+--------------+\n",
      "| I-HumanCount   |     942 | 3.47%        |\n",
      "+----------------+---------+--------------+\n",
      "| I-Location     |    2062 | 7.59%        |\n",
      "+----------------+---------+--------------+\n",
      "| I-Medication   |     441 | 1.62%        |\n",
      "+----------------+---------+--------------+\n",
      "| I-Organization |    5773 | 21.26%       |\n",
      "+----------------+---------+--------------+\n",
      "| I-Pathogen     |     463 | 1.71%        |\n",
      "+----------------+---------+--------------+\n",
      "| I-Symptom      |     311 | 1.15%        |\n",
      "+----------------+---------+--------------+\n",
      "\n",
      "üìä Validation entity distribution:\n",
      "+----------------+---------+--------------+\n",
      "| Entity         |   Count | Percentage   |\n",
      "+================+=========+==============+\n",
      "| B-Date         |     124 | 3.71%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Disease      |     286 | 8.55%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-HumanCount   |      78 | 2.33%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Location     |     312 | 9.33%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Medication   |      17 | 0.51%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Organization |     304 | 9.09%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Pathogen     |      57 | 1.70%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Symptom      |      45 | 1.35%        |\n",
      "+----------------+---------+--------------+\n",
      "| I-Date         |     416 | 12.44%       |\n",
      "+----------------+---------+--------------+\n",
      "| I-Disease      |     393 | 11.75%       |\n",
      "+----------------+---------+--------------+\n",
      "| I-HumanCount   |     113 | 3.38%        |\n",
      "+----------------+---------+--------------+\n",
      "| I-Location     |     333 | 9.96%        |\n",
      "+----------------+---------+--------------+\n",
      "| I-Medication   |      48 | 1.43%        |\n",
      "+----------------+---------+--------------+\n",
      "| I-Organization |     710 | 21.23%       |\n",
      "+----------------+---------+--------------+\n",
      "| I-Pathogen     |      67 | 2.00%        |\n",
      "+----------------+---------+--------------+\n",
      "| I-Symptom      |      42 | 1.26%        |\n",
      "+----------------+---------+--------------+\n",
      "\n",
      "üìä Test entity distribution:\n",
      "+----------------+---------+--------------+\n",
      "| Entity         |   Count | Percentage   |\n",
      "+================+=========+==============+\n",
      "| B-Date         |     154 | 4.90%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Disease      |     287 | 9.13%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-HumanCount   |      91 | 2.89%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Location     |     261 | 8.30%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Medication   |      20 | 0.64%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Organization |     269 | 8.55%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Pathogen     |      46 | 1.46%        |\n",
      "+----------------+---------+--------------+\n",
      "| B-Symptom      |      32 | 1.02%        |\n",
      "+----------------+---------+--------------+\n",
      "| I-Date         |     442 | 14.05%       |\n",
      "+----------------+---------+--------------+\n",
      "| I-Disease      |     420 | 13.35%       |\n",
      "+----------------+---------+--------------+\n",
      "| I-HumanCount   |     131 | 4.17%        |\n",
      "+----------------+---------+--------------+\n",
      "| I-Location     |     223 | 7.09%        |\n",
      "+----------------+---------+--------------+\n",
      "| I-Medication   |      47 | 1.49%        |\n",
      "+----------------+---------+--------------+\n",
      "| I-Organization |     650 | 20.67%       |\n",
      "+----------------+---------+--------------+\n",
      "| I-Pathogen     |      50 | 1.59%        |\n",
      "+----------------+---------+--------------+\n",
      "| I-Symptom      |      22 | 0.70%        |\n",
      "+----------------+---------+--------------+\n",
      "üíæ Split information saved to /home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/Models/bert_khmer_ner_model/split_info.json\n",
      "ü§ñ Loading model: GKLMIP/bert-khmer-small-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at GKLMIP/bert-khmer-small-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model and tokenizer loaded successfully\n",
      "üîÑ Preparing datasets...\n",
      "üîÑ Preparing dataset...\n",
      "‚úÖ Prepared 4976 examples, skipped 0\n",
      "üîÑ Preparing dataset...\n",
      "‚úÖ Prepared 622 examples, skipped 0\n",
      "üîÑ Preparing dataset...\n",
      "‚úÖ Prepared 623 examples, skipped 0\n",
      "üéì Starting model training...\n",
      "üñ•Ô∏è  Using device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "   Memory: 16.9 GB\n",
      "üöÄ Starting training for Khmer NER...\n",
      "   Epochs: 10\n",
      "   Per-device batch size: 8\n",
      "   Gradient accumulation steps: 2\n",
      "   Effective batch size: 16\n",
      "   Learning rate: 2e-05\n",
      "   Early stopping patience: 3 epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2177' max='3110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2177/3110 00:33 < 00:14, 65.91 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.207286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.189400</td>\n",
       "      <td>0.172495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.145300</td>\n",
       "      <td>0.153441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.148791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>0.150308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.098800</td>\n",
       "      <td>0.151319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.092700</td>\n",
       "      <td>0.153856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training completed!\n",
      "üíæ Final model saved to /home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/Models/bert_khmer_ner_model/final_model\n",
      "üìä Starting model evaluation...\n",
      "üîç Evaluating model on test set...\n",
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00<00:00, 455.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation completed on 623 samples\n",
      "üíæ Detailed predictions saved to /home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/Models/bert_khmer_ner_model/predictions.json\n",
      "üíæ CoNLL format saved to /home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/Models/bert_khmer_ner_model/predictions.conll\n",
      "\n",
      "üìä Comprehensive NER Evaluation Results:\n",
      "================================================================================\n",
      "üéØ ENTITY-LEVEL METRICS (Primary - Standard for NER)\n",
      "--------------------------------------------------\n",
      "Entity Precision: 0.6575\n",
      "Entity Recall:    0.7595\n",
      "Entity F1-Score:  0.7048\n",
      "Entity Accuracy:  0.9531\n",
      "\n",
      "üìã Entity-level Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Symptom       0.37      0.59      0.46        32\n",
      "Organization       0.53      0.66      0.59       270\n",
      "    Location       0.72      0.82      0.77       260\n",
      "        Date       0.82      0.84      0.83       154\n",
      "     Disease       0.73      0.80      0.77       287\n",
      "  HumanCount       0.71      0.84      0.77        91\n",
      "  Medication       0.00      0.00      0.00        20\n",
      "    Pathogen       0.60      0.72      0.65        46\n",
      "\n",
      "   micro avg       0.66      0.76      0.70      1160\n",
      "   macro avg       0.67      0.76      0.71      1160\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üî§ TOKEN-LEVEL METRICS (Secondary)\n",
      "--------------------------------------------------\n",
      "Token Accuracy: 0.9531\n",
      "Token Precision (Micro): 0.9531\n",
      "Token Recall (Micro):    0.9531\n",
      "Token F1-Score (Micro):  0.9531\n",
      "Token Precision (Macro): 0.7441\n",
      "Token Recall (Macro):    0.7992\n",
      "Token F1-Score (Macro):  0.7602\n",
      "\n",
      "üìã Token-level Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        B-Date       0.92      0.95      0.93       153\n",
      "     B-Disease       0.86      0.89      0.87       286\n",
      "  B-HumanCount       0.73      0.87      0.79        86\n",
      "    B-Location       0.81      0.86      0.83       260\n",
      "  B-Medication       0.50      0.20      0.29        20\n",
      "B-Organization       0.77      0.79      0.78       268\n",
      "    B-Pathogen       0.72      0.83      0.77        46\n",
      "     B-Symptom       0.47      0.72      0.57        32\n",
      "        I-Date       0.96      0.95      0.96       376\n",
      "     I-Disease       0.85      0.88      0.87       669\n",
      "  I-HumanCount       0.86      0.91      0.88       190\n",
      "    I-Location       0.80      0.87      0.83       286\n",
      "  I-Medication       0.61      0.56      0.58        81\n",
      "I-Organization       0.78      0.78      0.78       788\n",
      "    I-Pathogen       0.70      0.78      0.74       104\n",
      "     I-Symptom       0.33      0.78      0.47        40\n",
      "             O       0.98      0.97      0.98     20457\n",
      "\n",
      "      accuracy                           0.95     24142\n",
      "     macro avg       0.74      0.80      0.76     24142\n",
      "  weighted avg       0.96      0.95      0.95     24142\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìà ADDITIONAL ANALYSIS\n",
      "--------------------------------------------------\n",
      "Total tokens: 24,142\n",
      "Entity tokens: 3,685 (15.3%)\n",
      "O tokens: 20,457 (84.7%)\n",
      "Total sequences: 623\n",
      "\n",
      "üè∑Ô∏è Entity Type Distribution:\n",
      "  I-Organization: 788\n",
      "  I-Disease: 669\n",
      "  I-Date: 376\n",
      "  I-Location: 286\n",
      "  B-Disease: 286\n",
      "  B-Organization: 268\n",
      "  B-Location: 260\n",
      "  I-HumanCount: 190\n",
      "  B-Date: 153\n",
      "  I-Pathogen: 104\n",
      "  B-HumanCount: 86\n",
      "  I-Medication: 81\n",
      "  B-Pathogen: 46\n",
      "  I-Symptom: 40\n",
      "  B-Symptom: 32\n",
      "  B-Medication: 20\n",
      "\n",
      "üíæ Comprehensive metrics saved to /home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/Models/bert_khmer_ner_model/evaluation_metrics.json\n",
      "üíæ Label mappings saved to /home/guest/Public/KHEED/KHEED_Data_Collection/Evaluation/Models/bert_khmer_ner_model\n",
      "üéâ Fine-tuning and evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"üì• Loading data...\")\n",
    "# If load_json_files_robust uses glob, make sure to use glob.glob, not glob()\n",
    "all_tokens, all_tags, input_files = load_json_files_robust(input_dir)\n",
    "\n",
    "if not all_tokens:\n",
    "    print(\"‚ùå No data loaded. Please check your input directory and file format.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"‚úÖ Successfully loaded {len(all_tokens)} sentences from {len(input_files)} files\")\n",
    "\n",
    "# Check for duplicate sentences to prevent data leakage\n",
    "sentence_strings = [\" \".join(tokens) for tokens in all_tokens]\n",
    "duplicate_counts = Counter(sentence_strings)\n",
    "duplicates = {sent: count for sent, count in duplicate_counts.items() if count > 1}\n",
    "if duplicates:\n",
    "    print(f\"‚ö†Ô∏è Warning: {len(duplicates)} duplicate sentences found:\")\n",
    "    for sent, count in list(duplicates.items())[:5]:  # Show up to 5 duplicates\n",
    "        print(f\"  Sentence: '{sent[:50]}...' (repeated {count} times)\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicate sentences found.\")\n",
    "\n",
    "# Set random seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Create label mappings\n",
    "unique_labels = set(tag for tags in all_tags for tag in tags)\n",
    "unique_labels.add('O')\n",
    "sorted_labels = sorted(unique_labels)\n",
    "label2id = {label: idx for idx, label in enumerate(sorted_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"üè∑Ô∏è Label mapping created:\")\n",
    "print(f\"  Total labels: {len(label2id)}\")\n",
    "print(f\"  Labels: {sorted_labels}\")\n",
    "\n",
    "# Function to print entity distribution\n",
    "def print_entity_distribution(tags, name):\n",
    "    entity_counts = Counter(tag for sentence in tags for tag in sentence if tag != 'O')\n",
    "    total_entities = sum(entity_counts.values())\n",
    "    print(f\"\\nüìä {name} entity distribution:\")\n",
    "    table_data = [[tag, count, f\"{(count/total_entities)*100:.2f}%\"] for tag, count in sorted(entity_counts.items())]\n",
    "    headers = ['Entity', 'Count', 'Percentage']\n",
    "    print(tabulate(table_data, headers=headers, tablefmt='grid'))\n",
    "\n",
    "\n",
    "# Split data\n",
    "train_tokens, temp_tokens, train_tags, temp_tags = train_test_split(\n",
    "    all_tokens, all_tags, test_size=0.2, random_state=seed, stratify=None\n",
    ")\n",
    "\n",
    "val_tokens, test_tokens, val_tags, test_tags = train_test_split(\n",
    "    temp_tokens, temp_tags, test_size=0.5, random_state=seed, stratify=None\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data split:\")\n",
    "print(f\"  Training: {len(train_tokens):,} sentences\")\n",
    "print(f\"  Validation: {len(val_tokens):,} sentences\")\n",
    "print(f\"  Test: {len(test_tokens):,} sentences\")\n",
    "\n",
    "# Verify entity distribution in each split\n",
    "print_entity_distribution(train_tags, \"Training\")\n",
    "print_entity_distribution(val_tags, \"Validation\")\n",
    "print_entity_distribution(test_tags, \"Test\")\n",
    "\n",
    "# Save split information\n",
    "split_info = {\n",
    "    \"train_sentences\": len(train_tokens),\n",
    "    \"val_sentences\": len(val_tokens),\n",
    "    \"test_sentences\": len(test_tokens),\n",
    "    \"train_entity_distribution\": dict(Counter(tag for sentence in train_tags for tag in sentence if tag != 'O')),\n",
    "    \"val_entity_distribution\": dict(Counter(tag for sentence in val_tags for tag in sentence if tag != 'O')),\n",
    "    \"test_entity_distribution\": dict(Counter(tag for sentence in test_tags for tag in sentence if tag != 'O'))\n",
    "}\n",
    "split_info_path = os.path.join(output_dir, \"split_info.json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(split_info_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(split_info, f, ensure_ascii=False, indent=2)\n",
    "print(f\"üíæ Split information saved to {split_info_path}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"GKLMIP/bert-khmer-small-uncased\"\n",
    "print(f\"ü§ñ Loading model: {model_name}\")\n",
    "\n",
    "try:\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "    model = BertForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(label2id),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    print(\"‚úÖ Model and tokenizer loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Prepare datasets\n",
    "print(\"üîÑ Preparing datasets...\")\n",
    "train_encodings = prepare_dataset_robust(train_tokens, train_tags, tokenizer, label2id)\n",
    "val_encodings = prepare_dataset_robust(val_tokens, val_tags, tokenizer, label2id)\n",
    "test_encodings = prepare_dataset_robust(test_tokens, test_tags, tokenizer, label2id)\n",
    "\n",
    "train_dataset = NERDataset(train_encodings)\n",
    "val_dataset = NERDataset(val_encodings)\n",
    "test_dataset = NERDataset(test_encodings)\n",
    "\n",
    "# Fine-tune model\n",
    "print(\"üéì Starting model training...\")\n",
    "trained_model = train_model_trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    output_dir=output_dir,\n",
    "    tokenizer=tokenizer,\n",
    "    num_epochs=10,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"üìä Starting model evaluation...\")\n",
    "predictions, true_tags, pred_tags, metrics = evaluate_model_standard(\n",
    "    model=trained_model,\n",
    "    tokenizer=tokenizer,\n",
    "    test_dataset=test_dataset,\n",
    "    id2label=id2label,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Save label mappings\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(os.path.join(output_dir, \"label2id.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(label2id, f, ensure_ascii=False, indent=2)\n",
    "with open(os.path.join(output_dir, \"id2label.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(id2label, f, ensure_ascii=False, indent=2)\n",
    "print(f\"üíæ Label mappings saved to {output_dir}\")\n",
    "\n",
    "print(\"üéâ Fine-tuning and evaluation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kheed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
